[
  {
    "objectID": "medrev.html",
    "href": "medrev.html",
    "title": "MedScrape: AI-Powered Literature Review Platform",
    "section": "",
    "text": "MedScrape is an advanced literature review platform that leverages natural language processing and machine learning to automate the tedious process of conducting systematic literature reviews. It scrapes PubMed, clusters research articles using multiple unsupervised learning algorithms, and identifies research gaps by comparing review papers against primary research.\nRepository: github.com/lonespear/medrev\nLive Demo: medreview.streamlit.app\nResearch Paper: “Literature Reviews in the Age of Precision Nutrition”"
  },
  {
    "objectID": "medrev.html#overview",
    "href": "medrev.html#overview",
    "title": "MedScrape: AI-Powered Literature Review Platform",
    "section": "",
    "text": "MedScrape is an advanced literature review platform that leverages natural language processing and machine learning to automate the tedious process of conducting systematic literature reviews. It scrapes PubMed, clusters research articles using multiple unsupervised learning algorithms, and identifies research gaps by comparing review papers against primary research.\nRepository: github.com/lonespear/medrev\nLive Demo: medreview.streamlit.app\nResearch Paper: “Literature Reviews in the Age of Precision Nutrition”"
  },
  {
    "objectID": "medrev.html#key-features",
    "href": "medrev.html#key-features",
    "title": "MedScrape: AI-Powered Literature Review Platform",
    "section": "Key Features",
    "text": "Key Features\n\n1. Automated PubMed Database Scraping\nSeamlessly interfaces with the NCBI Entrez API to retrieve research articles:\n\nAdvanced Query Builder: Boolean operators (AND, OR, NOT) with nested queries\nDate Range Filtering: Target specific publication periods\nPublication Type Filtering: Separate reviews from research articles\nCustom CSV Upload: Analyze your own literature collections\nAutomatic Article Classification: Identifies review vs. research papers\n\n# Example PubMed query construction\ndef search_pubmed(query, start_date, end_date, max_results=1000):\n    \"\"\"\n    Search PubMed with advanced filtering\n\n    Parameters:\n    -----------\n    query : str\n        Search terms with boolean operators\n        Example: '(\"genetics\" OR \"GWAS\") AND (\"nutrition\" OR \"diet\")'\n    start_date : str\n        Start date (YYYY/MM/DD)\n    end_date : str\n        End date (YYYY/MM/DD)\n    max_results : int\n        Maximum articles to retrieve\n\n    Returns:\n    --------\n    articles : list\n        List of article dictionaries with metadata\n    \"\"\"\n    from Bio import Entrez\n\n    Entrez.email = \"your.email@example.com\"\n\n    # Construct date-filtered query\n    date_filter = f\"{start_date}:{end_date}[PDAT]\"\n    full_query = f\"({query}) AND ({date_filter})\"\n\n    # Search PubMed\n    handle = Entrez.esearch(\n        db=\"pubmed\",\n        term=full_query,\n        retmax=max_results,\n        sort=\"relevance\"\n    )\n    record = Entrez.read(handle)\n    pmids = record[\"IdList\"]\n\n    # Fetch article details\n    handle = Entrez.efetch(\n        db=\"pubmed\",\n        id=pmids,\n        rettype=\"medline\",\n        retmode=\"text\"\n    )\n\n    # Parse results\n    articles = parse_medline_records(handle)\n\n    return articles\n\n\n2. Multiple Clustering Algorithms\nEmploys diverse unsupervised learning techniques to discover thematic patterns:\n\nK-Means Clustering\n\nBest For: Clear, well-separated topics\nAdvantages: Fast, deterministic, interpretable\nUse Case: Initial exploration, known number of topics\n\nfrom sklearn.cluster import KMeans\n\n# K-Means clustering\nkmeans = KMeans(\n    n_clusters=8,\n    init='k-means++',\n    n_init=10,\n    max_iter=300,\n    random_state=42\n)\ncluster_labels = kmeans.fit_predict(document_vectors)\n\n\nDBSCAN (Density-Based Spatial Clustering)\n\nBest For: Irregular cluster shapes, outlier detection\nAdvantages: No predefined cluster count, noise robust\nUse Case: Discovering organic topic structures\n\nfrom sklearn.cluster import DBSCAN\n\n# DBSCAN clustering\ndbscan = DBSCAN(\n    eps=0.5,            # Maximum distance between points\n    min_samples=5,      # Minimum cluster size\n    metric='cosine'     # Similarity metric\n)\ncluster_labels = dbscan.fit_predict(document_vectors)\n\n\nHierarchical Clustering\n\nBest For: Taxonomic relationships, nested topics\nAdvantages: Creates dendrogram, multi-scale analysis\nUse Case: Understanding topic hierarchies\n\nfrom sklearn.cluster import AgglomerativeClustering\n\n# Hierarchical clustering\nhierarchical = AgglomerativeClustering(\n    n_clusters=8,\n    linkage='ward',     # Minimizes variance\n    affinity='euclidean'\n)\ncluster_labels = hierarchical.fit_predict(document_vectors)\n\n\nLDA (Latent Dirichlet Allocation)\n\nBest For: Topic modeling, document-topic distributions\nAdvantages: Probabilistic, interpretable topics\nUse Case: Understanding document composition\n\nfrom sklearn.decomposition import LatentDirichletAllocation\n\n# LDA topic modeling\nlda = LatentDirichletAllocation(\n    n_components=8,\n    max_iter=10,\n    learning_method='online',\n    random_state=42\n)\ntopic_distributions = lda.fit_transform(document_term_matrix)\n\n\n\n3. Flexible Embedding Methods\nThree approaches to convert documents into numerical vectors:\n\nTF-IDF (Term Frequency-Inverse Document Frequency)\n\nSpeed: Fastest\nMemory: Minimal\nBest For: Large corpora, quick analysis\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\ntfidf = TfidfVectorizer(\n    max_features=5000,\n    min_df=2,\n    max_df=0.8,\n    ngram_range=(1, 2),\n    stop_words='english'\n)\nvectors = tfidf.fit_transform(abstracts)\n\n\nDoc2Vec\n\nSpeed: Moderate\nMemory: Moderate\nBest For: Context-aware representations\n\nfrom gensim.models.doc2vec import Doc2Vec, TaggedDocument\n\n# Prepare documents\ntagged_docs = [TaggedDocument(words=doc.split(), tags=[str(i)])\n               for i, doc in enumerate(abstracts)]\n\n# Train Doc2Vec model\nmodel = Doc2Vec(\n    tagged_docs,\n    vector_size=100,\n    window=5,\n    min_count=2,\n    epochs=40,\n    dm=1  # Distributed memory\n)\n\n# Get document vectors\nvectors = [model.dv[str(i)] for i in range(len(abstracts))]\n\n\nFastText\n\nSpeed: Slower\nMemory: Higher\nBest For: Robust to typos, rare words\n\nfrom gensim.models.fasttext import FastText\n\n# Train FastText model\nsentences = [doc.split() for doc in abstracts]\nmodel = FastText(\n    sentences,\n    vector_size=100,\n    window=5,\n    min_count=2,\n    epochs=10,\n    word_ngrams=1\n)\n\n# Average word vectors for document representation\nvectors = []\nfor doc in sentences:\n    doc_vector = np.mean([model.wv[word] for word in doc\n                          if word in model.wv], axis=0)\n    vectors.append(doc_vector)\n\n\n\n4. Dimensionality Reduction\nReduces high-dimensional embeddings for visualization and analysis:\n\nPCA (Principal Component Analysis)\n\nType: Linear\nSpeed: Fastest\nBest For: Initial exploration\n\nfrom sklearn.decomposition import PCA\n\npca = PCA(n_components=3)\nreduced = pca.fit_transform(vectors)\n\n\nt-SNE (t-Distributed Stochastic Neighbor Embedding)\n\nType: Non-linear\nSpeed: Moderate\nBest For: Detailed cluster visualization\n\nfrom sklearn.manifold import TSNE\n\ntsne = TSNE(\n    n_components=3,\n    perplexity=30,\n    n_iter=1000,\n    random_state=42\n)\nreduced = tsne.fit_transform(vectors)\n\n\nUMAP (Uniform Manifold Approximation and Projection)\n\nType: Non-linear\nSpeed: Fast\nBest For: Best balance of speed and quality\n\nimport umap\n\nreducer = umap.UMAP(\n    n_components=3,\n    n_neighbors=15,\n    min_dist=0.1,\n    metric='cosine'\n)\nreduced = reducer.fit_transform(vectors)\n\n\n\n5. Research Gap Identification\nNovel Feature: Compares review papers against primary research to find underexplored areas.\n\nAlgorithm\ndef identify_research_gaps(review_clusters, research_clusters, threshold=0.3):\n    \"\"\"\n    Identify research gaps by comparing review and research clusters\n\n    Parameters:\n    -----------\n    review_clusters : dict\n        {cluster_id: [review_article_indices]}\n    research_clusters : dict\n        {cluster_id: [research_article_indices]}\n    threshold : float\n        Minimum similarity to consider coverage\n\n    Returns:\n    --------\n    gaps : dict\n        {research_cluster_id: coverage_metrics}\n    \"\"\"\n    # Extract cluster centroids\n    review_centroids = compute_centroids(review_clusters)\n    research_centroids = compute_centroids(research_clusters)\n\n    # Compute similarity matrix\n    similarity_matrix = cosine_similarity(\n        research_centroids,\n        review_centroids\n    )\n\n    # Identify gaps\n    gaps = {}\n    for i, research_cluster in enumerate(research_clusters):\n        max_similarity = np.max(similarity_matrix[i])\n\n        if max_similarity &lt; threshold:\n            # This research cluster is underreviewed\n            gaps[research_cluster] = {\n                'coverage': max_similarity,\n                'article_count': len(research_clusters[research_cluster]),\n                'top_terms': extract_top_terms(research_cluster),\n                'representative_articles': get_representative_articles(\n                    research_cluster, n=3\n                )\n            }\n\n    return gaps\n\n\nCoverage Heatmap\nimport plotly.express as px\n\ndef plot_coverage_heatmap(similarity_matrix, review_labels, research_labels):\n    \"\"\"\n    Visualize coverage of research topics by reviews\n    \"\"\"\n    fig = px.imshow(\n        similarity_matrix,\n        x=review_labels,\n        y=research_labels,\n        color_continuous_scale='RdYlGn',\n        aspect='auto',\n        labels={'x': 'Review Topics', 'y': 'Research Topics', 'color': 'Coverage'}\n    )\n\n    fig.update_layout(\n        title='Research Coverage by Review Papers',\n        xaxis_title='Review Clusters',\n        yaxis_title='Research Clusters'\n    )\n\n    return fig\n\n\n\n6. Interactive 3D Visualizations\nExplore document clusters in three dimensions:\nimport plotly.graph_objects as go\n\ndef create_3d_scatter(reduced_vectors, cluster_labels, titles):\n    \"\"\"\n    Create interactive 3D scatter plot\n    \"\"\"\n    fig = go.Figure()\n\n    for cluster_id in np.unique(cluster_labels):\n        mask = cluster_labels == cluster_id\n        fig.add_trace(go.Scatter3d(\n            x=reduced_vectors[mask, 0],\n            y=reduced_vectors[mask, 1],\n            z=reduced_vectors[mask, 2],\n            mode='markers',\n            name=f'Cluster {cluster_id}',\n            text=[titles[i] for i in np.where(mask)[0]],\n            hovertemplate='&lt;b&gt;%{text}&lt;/b&gt;&lt;br&gt;' +\n                         'X: %{x:.2f}&lt;br&gt;' +\n                         'Y: %{y:.2f}&lt;br&gt;' +\n                         'Z: %{z:.2f}&lt;extra&gt;&lt;/extra&gt;',\n            marker=dict(size=5, opacity=0.7)\n        ))\n\n    fig.update_layout(\n        title='Literature Clustering (3D)',\n        scene=dict(\n            xaxis_title='Dimension 1',\n            yaxis_title='Dimension 2',\n            zaxis_title='Dimension 3'\n        ),\n        height=700\n    )\n\n    return fig\n\n\n7. Extractive Summarization\nAutomatically generates cluster summaries:\ndef summarize_cluster(cluster_documents, n_terms=10):\n    \"\"\"\n    Extract key terms representing cluster theme\n\n    Uses TF-IDF to identify most discriminative terms\n    \"\"\"\n    # Calculate TF-IDF for cluster documents\n    tfidf = TfidfVectorizer(max_features=100, stop_words='english')\n    tfidf_matrix = tfidf.fit_transform(cluster_documents)\n\n    # Get feature names and scores\n    feature_names = tfidf.get_feature_names_out()\n    scores = np.asarray(tfidf_matrix.sum(axis=0)).flatten()\n\n    # Sort by score\n    top_indices = np.argsort(scores)[::-1][:n_terms]\n    top_terms = [feature_names[i] for i in top_indices]\n\n    return top_terms"
  },
  {
    "objectID": "medrev.html#system-architecture",
    "href": "medrev.html#system-architecture",
    "title": "MedScrape: AI-Powered Literature Review Platform",
    "section": "System Architecture",
    "text": "System Architecture\n┌──────────────────────────────────────────────┐\n│           Streamlit Web Interface            │\n│  (User Input, Visualization, Export)         │\n└────────────────┬─────────────────────────────┘\n                 │\n                 ↓\n┌──────────────────────────────────────────────┐\n│         PubMed Scraper Module                │\n│  • NCBI Entrez API Integration               │\n│  • Query Construction                        │\n│  • Metadata Extraction                       │\n└────────────────┬─────────────────────────────┘\n                 │\n                 ↓\n┌──────────────────────────────────────────────┐\n│       Enhanced Clusterer Engine              │\n├──────────────────────────────────────────────┤\n│  Embedding Layer:                            │\n│  • TF-IDF Vectorizer                         │\n│  • Doc2Vec Model                             │\n│  • FastText Model                            │\n│                                              │\n│  Clustering Layer:                           │\n│  • K-Means                                   │\n│  • DBSCAN                                    │\n│  • Hierarchical                              │\n│  • LDA                                       │\n│                                              │\n│  Reduction Layer:                            │\n│  • PCA                                       │\n│  • t-SNE                                     │\n│  • UMAP                                      │\n└────────────────┬─────────────────────────────┘\n                 │\n                 ↓\n┌──────────────────────────────────────────────┐\n│      Review Comparator Module                │\n│  • Cluster Centroid Computation              │\n│  • Cosine Similarity Matrix                  │\n│  • Gap Threshold Analysis                    │\n│  • Coverage Metrics                          │\n└────────────────┬─────────────────────────────┘\n                 │\n                 ↓\n┌──────────────────────────────────────────────┐\n│      Visualization & Export                  │\n│  • Plotly Interactive Plots                  │\n│  • CSV Download                              │\n│  • Gap Reports                               │\n└──────────────────────────────────────────────┘"
  },
  {
    "objectID": "medrev.html#technical-stack",
    "href": "medrev.html#technical-stack",
    "title": "MedScrape: AI-Powered Literature Review Platform",
    "section": "Technical Stack",
    "text": "Technical Stack\n\n\n\n\n\n\n\n\nComponent\nTechnology\nPurpose\n\n\n\n\nWeb Framework\nStreamlit\nInteractive application\n\n\nDatabase Access\nBiopython (Entrez)\nPubMed API integration\n\n\nNLP Embeddings\nGensim (Doc2Vec, FastText)\nDocument representations\n\n\nText Vectorization\nscikit-learn (TF-IDF)\nTraditional embeddings\n\n\nClustering\nscikit-learn\nUnsupervised learning\n\n\nDimensionality Reduction\nUMAP, t-SNE, PCA\nVisualization\n\n\nVisualization\nPlotly\nInteractive 3D plots\n\n\nData Manipulation\nPandas, NumPy\nData processing"
  },
  {
    "objectID": "medrev.html#use-cases",
    "href": "medrev.html#use-cases",
    "title": "MedScrape: AI-Powered Literature Review Platform",
    "section": "Use Cases",
    "text": "Use Cases\n\n1. Precision Nutrition Research\nQuery:\n(\"genetics\" OR \"GWAS\" OR \"genomic\") AND (\"nutrition\" OR \"diet\" OR \"metabolism\")\nWorkflow: 1. Scrape 500 articles from last 5 years 2. Separate reviews (n=50) from research (n=450) 3. Cluster research into 8 topics 4. Cluster reviews into 4 topics 5. Identify gaps: research clusters not covered by reviews\nResult: Discovered underreviewed area in “nutrigenomics of microbiome interactions”\n\n\n2. Exercise Physiology\nQuery:\n(\"genes\" OR \"genetics\") AND (\"exercise\" OR \"physical activity\" OR \"training\")\nAnalysis: - K-Means clustering reveals 6 main themes - LDA topic modeling identifies “exercise genetics” vs “training adaptation” - Gap analysis shows need for reviews on “epigenetic exercise responses”\n\n\n3. Clinical Medicine\nQuery:\n(\"diabetes\" OR \"obesity\") AND (\"treatment\" OR \"intervention\") AND (\"randomized controlled trial\"[PT])\nInsights: - Hierarchical clustering shows treatment taxonomy - DBSCAN identifies outlier studies (novel interventions) - Coverage heatmap highlights gaps in “combination therapy reviews”"
  },
  {
    "objectID": "medrev.html#installation-usage",
    "href": "medrev.html#installation-usage",
    "title": "MedScrape: AI-Powered Literature Review Platform",
    "section": "Installation & Usage",
    "text": "Installation & Usage\n\nInstallation\n# Clone repository\ngit clone https://github.com/lonespear/medrev.git\ncd medrev\n\n# Install dependencies\npip install -r requirements.txt\nRequirements:\nstreamlit&gt;=1.20.0\nbiopython&gt;=1.79\nscikit-learn&gt;=1.0.0\ngensim&gt;=4.0.0\numap-learn&gt;=0.5.0\nplotly&gt;=5.0.0\npandas&gt;=1.3.0\nnumpy&gt;=1.20.0\n\n\nRunning Locally\nstreamlit run app_enhanced.py\n\n\nAccess Live Demo\nVisit: https://medreview.streamlit.app/\n\n\nProgrammatic Usage\nfrom clustering_enhanced import EnhancedClusterer, ReviewComparator\n\n# Load your article data\narticles_df = pd.read_csv('pubmed_articles.csv')\n\n# Initialize clusterer\nclusterer = EnhancedClusterer(\n    embedding_method='doc2vec',\n    clustering_method='kmeans',\n    n_clusters=8,\n    reduction_method='umap'\n)\n\n# Fit and transform\nresults = clusterer.fit_transform(\n    documents=articles_df['abstract'].tolist(),\n    titles=articles_df['title'].tolist()\n)\n\n# Identify gaps\ncomparator = ReviewComparator()\ngaps = comparator.find_gaps(\n    review_articles=review_df,\n    research_articles=research_df,\n    threshold=0.3\n)\n\n# Visualize\nfig = clusterer.plot_3d_clusters()\nfig.show()\n\n# Export\nresults_df = clusterer.export_results()\nresults_df.to_csv('cluster_results.csv')"
  },
  {
    "objectID": "medrev.html#performance-metrics",
    "href": "medrev.html#performance-metrics",
    "title": "MedScrape: AI-Powered Literature Review Platform",
    "section": "Performance Metrics",
    "text": "Performance Metrics\n\nProcessing Speed\n\n\n\nCorpus Size\nEmbedding\nClustering\nTotal Time\n\n\n\n\n100 articles\n5s\n1s\n~6s\n\n\n500 articles\n20s\n3s\n~25s\n\n\n1,000 articles\n45s\n8s\n~55s\n\n\n5,000 articles\n4min\n40s\n~5min\n\n\n10,000 articles\n8min\n2min\n~10min\n\n\n\nTested on Intel i7-9700K, 32GB RAM\n\n\nMemory Usage\n\nTF-IDF: ~50MB for 1,000 articles\nDoc2Vec: ~200MB for 1,000 articles\nFastText: ~500MB for 1,000 articles\n\n\n\nClustering Quality (Silhouette Score)\n\n\n\nMethod\nAvg Score\nBest Use Case\n\n\n\n\nK-Means\n0.45\nWell-separated topics\n\n\nDBSCAN\n0.38\nIrregular clusters\n\n\nHierarchical\n0.42\nNested topics\n\n\nLDA\n0.40\nTopic modeling"
  },
  {
    "objectID": "medrev.html#research-paper",
    "href": "medrev.html#research-paper",
    "title": "MedScrape: AI-Powered Literature Review Platform",
    "section": "Research Paper",
    "text": "Research Paper\nTitle: Literature Reviews in the Age of Precision Nutrition\nAuthors: - Jake R. Beckman - Jon Day - Russell Nelson - Quinten Weeks - Joseph Dorta - Moussa Doumbia - Diana M. Thomas\nInstitution: Department of Mathematical Sciences, United States Military Academy, West Point, NY\nFunding: - Nutrition for Precision Health (NPH) Initiative - All of Us Research Program - NIH Common Fund\nAbstract: This paper introduces MedScrape, a novel platform for automated literature review and research gap identification. We demonstrate its effectiveness in the domain of precision nutrition, showing how unsupervised learning can systematically identify underreviewed research areas. Our methodology combines PubMed scraping, multiple clustering algorithms, and cosine similarity-based gap detection to provide researchers with actionable insights for future review articles.\nContact: Dr. Diana Thomas (diana.thomas@westpoint.edu)"
  },
  {
    "objectID": "medrev.html#future-enhancements",
    "href": "medrev.html#future-enhancements",
    "title": "MedScrape: AI-Powered Literature Review Platform",
    "section": "Future Enhancements",
    "text": "Future Enhancements\n\nPlanned Features\n\nAdditional Databases\n\nWeb of Science integration\nScopus support\narXiv for preprints\nbioRxiv for biology\n\nAdvanced Analytics\n\nCitation network analysis\nAuthor collaboration networks\nTemporal trend analysis\nJournal impact metrics\n\nAutomation\n\nAPI endpoints for programmatic access\nScheduled searches with email alerts\nAutomated gap report generation\nIntegration with reference managers (Zotero, Mendeley)\n\nEnhanced NLP\n\nTransformer-based embeddings (BERT, SciBERT)\nNamed entity recognition\nRelationship extraction\nMulti-language support\n\nCollaboration Features\n\nUser accounts and saved searches\nShared project workspaces\nAnnotation and tagging\nTeam collaboration tools"
  },
  {
    "objectID": "medrev.html#contributing",
    "href": "medrev.html#contributing",
    "title": "MedScrape: AI-Powered Literature Review Platform",
    "section": "Contributing",
    "text": "Contributing\nWe welcome contributions! Priority areas:\n\nPerformance Optimization: Speed up embedding generation\nNew Algorithms: Additional clustering methods\nDatabase Integration: Support for more literature databases\nVisualization: Enhanced plotting options\nDocumentation: Usage examples and tutorials\nTesting: Unit tests and integration tests"
  },
  {
    "objectID": "medrev.html#citation",
    "href": "medrev.html#citation",
    "title": "MedScrape: AI-Powered Literature Review Platform",
    "section": "Citation",
    "text": "Citation\nIf you use MedScrape in your research, please cite:\nBeckman, J. R., Day, J., Nelson, R., Weeks, Q., Dorta, J., Doumbia, M., & Thomas, D. M. (2024).\nLiterature Reviews in the Age of Precision Nutrition.\nDepartment of Mathematical Sciences, United States Military Academy, West Point, NY.\nBibTeX:\n@software{medreview2024,\n  author = {Beckman, Jake R. and Day, Jonathan and Nelson, Russell and\n            Weeks, Quinten and Dorta, Joseph and Doumbia, Moussa and Thomas, Diana M.},\n  title = {MedScrape: AI-Powered Literature Review Platform},\n  year = {2024},\n  publisher = {GitHub},\n  url = {https://github.com/lonespear/medrev}\n}"
  },
  {
    "objectID": "medrev.html#license",
    "href": "medrev.html#license",
    "title": "MedScrape: AI-Powered Literature Review Platform",
    "section": "License",
    "text": "License\nThis project is supported by the Nutrition for Precision Health Initiative and the All of Us Research Program."
  },
  {
    "objectID": "medrev.html#contact-support",
    "href": "medrev.html#contact-support",
    "title": "MedScrape: AI-Powered Literature Review Platform",
    "section": "Contact & Support",
    "text": "Contact & Support\nDevelopment Team: Department of Mathematical Sciences United States Military Academy West Point, NY 10996\nPrincipal Investigator: Dr. Diana M. Thomas Email: diana.thomas@westpoint.edu\nDeveloper: CPT Jonathan Day Email: jonathan.day@westpoint.edu\nRepository Issues: GitHub Issues\n\n← Back to Projects View on GitHub Try Live Demo"
  },
  {
    "objectID": "ma206_course.html",
    "href": "ma206_course.html",
    "title": "MA206: Introduction to Probability and Statistics",
    "section": "",
    "text": "To Cadets:\nThis course, MA206, introduces you to the foundational principles of probability and statistics, emphasizing data literacy and inference. It begins with Block I, covering data types, visualization, and basic probability rules including counting and the behavior of random variables. Block II builds on this by exploring discrete and continuous distributions, the Central Limit Theorem, and tools for one-sample inference such as confidence intervals and hypothesis testing for proportions and means. Finally, Block III develops cadets’ ability to analyze relationships between variables through two-sample tests, linear regression, ANOVA, and goodness-of-fit testing. By the end of the course, cadets will be equipped to make sound, data-driven decisions grounded in statistical reasoning.\nEvery statistical analysis begins with a clear, repeatable process that transforms curious questions into reliable insights. The Six Step Method serves as your roadmap—guiding you from formulating a meaningful research question through data collection, exploration, inference, and finally thoughtful reflection—so that every conclusion you draw is both rigorous and reproducible.\nSix Step Method\nA Note on Technology:\nIn this course the primary tool used for data analysis is R. Throughout this course you will implement techniques for summarizing, visualizing, and analyzing data. The primary focus of this course is not for you to become masters in coding, however building on skills learned in CY105 will help your analysis in understanding how to use information technology to demonstrate successful outcomes in this course."
  },
  {
    "objectID": "ma206_course.html#types-of-data-sampling-and-bias",
    "href": "ma206_course.html#types-of-data-sampling-and-bias",
    "title": "MA206: Introduction to Probability and Statistics",
    "section": "Types of Data, Sampling, and Bias",
    "text": "Types of Data, Sampling, and Bias\nFundamental to statistical analysis is understanding the types of data we encounter, the methods we use to collect them, and the potential sources of bias that can undermine the validity of our conclusions. We distinguish between categorical (qualitative) and quantitative (numerical) data. Categorical variables can further be broken down into nominal (color, baseball position, type of animal), ordinal (I had a very bad/somewhat bad/neutral/good/very good experience at CFT), and binary (Yes I passed Air Assault/No I did not). Quantitative data can also be broken down into either discrete or continuous. Understanding these distinctions is critical for selecting the correct tools for analysis and interpretation.\nThe two methods of sampling in this course will be through simple random sampling or convenience sampling. The difference in application is whether we can generalize our results to the larger population. As an example, say I do not have an exhaustive list of cadet ID numbers to randomly select from. Instead, I only can stand in Central Area after class at 1630 and survey the first 50 cadets that willingly take my survey. There is a certain sub-population I am probably missing (1st Reg, 4th Reg, Corps Squad, etc). This is a convenience sample. Instead, if I randomly select 50 C-Numbers from a complete list of the Corps obtained from the registrar, this would be a a true random sample of the Corps, whereas the former is what is known as selection bias.\nFinally, we explore the concept of bias in data collection. We identify common sources such as selection bias, response bias, and measurement bias, and discuss how poor sampling practices or flawed survey design can distort findings. This lesson sets the stage for the rest of the course by highlighting the importance of thoughtful data collection and critical evaluation of data sources.\nAs an example let us look at a dataset aggregated from over 50,000 diamonds:\n\n\n\n\n\ncarat\ncut\ncolor\nclarity\ndepth\ntable\nprice\nx\ny\nz\n\n\n\n\n1.02\nGood\nG\nVS2\n63.8\n59.0\n6080\n6.34\n6.27\n4.02\n\n\n0.31\nIdeal\nF\nVVS1\n61.9\n53.5\n882\n4.36\n4.39\n2.71\n\n\n0.60\nPremium\nD\nSI2\n61.3\n61.0\n1428\n5.46\n5.40\n3.33\n\n\n0.41\nIdeal\nE\nIF\n62.1\n54.0\n1419\n4.75\n4.81\n2.97\n\n\n0.72\nVery Good\nH\nVS1\n62.2\n54.0\n2877\n5.74\n5.76\n3.57\n\n\n1.20\nIdeal\nF\nVS2\n62.6\n56.0\n8486\n6.78\n6.73\n4.23\n\n\n\n\n\nEach of the rows is an individual diamond, generally called an observation. Each of the columns are unique aspects measured for every observation, called variables. Variables are either categorical, qualitative aspects of each measurement, or quantitative, a numbered entry."
  },
  {
    "objectID": "ma206_course.html#exploratory-data-analysis",
    "href": "ma206_course.html#exploratory-data-analysis",
    "title": "MA206: Introduction to Probability and Statistics",
    "section": "Exploratory Data Analysis",
    "text": "Exploratory Data Analysis\nUnderstanding, communicating, and interpreting your data is paramount to any initial data analysis project. These are done through numerous visualizations and summary statistics which we will learn to regularly implement when given any new dataset.\n\nOne Variable – Visualizations and Summary Statistics\nStarting with a variable-by-variable approach is a natural first step. This is done rapidly in R with the following few functions:\n\nHistograms\n\n\n\n\n\n\n\n\n\nHistograms tell us where most of the values for a quantitative variable lie in its given distribution. We can determine skewness, a measure of how lopsided the data appear or if there are any asymmetries or tails.\n\n\nBoxplots\n\n\n\n\n\n\n\n\n\nSimilar to a histogram, a boxplot will tell us exactly where the median, 1st and 3rd quartiles, and outliers exist for any quantitative variable. The ‘whiskers’ are determined by \\(1.5 \\times IQR\\) where the inter-quartile range is the \\(3rd - 1st\\) quartiles.\n\n\nSummary Statistics\n\n\n\n\n\nVariable\nMean\nMedian\nSD\nVar\nMin\nMax\n\n\n\n\ncarat\n0.79\n0.70\n0.46\n0.21\n0.23\n3.24\n\n\ndepth\n61.77\n61.90\n1.40\n1.97\n56.30\n68.90\n\n\ntable\n57.48\n57.00\n2.26\n5.11\n50.00\n66.00\n\n\nprice\n3873.55\n2387.00\n3912.55\n15308046.06\n337.00\n18692.00\n\n\nx\n5.72\n5.68\n1.10\n1.21\n3.88\n9.44\n\n\ny\n5.72\n5.68\n1.09\n1.20\n3.90\n9.40\n\n\nz\n3.53\n3.52\n0.68\n0.46\n2.39\n5.85\n\n\n\n\n\nThe above are the predominant statistics you want to discern for every quantitative variable in your dataset. The benchmark location statistics are the mean, median, max, and min, while the standard deviation and variance are measures of how spread out the data are relative to one another.\n\\[\n\\begin{align}\n\\text{Sample Mean: } \\ \\ & \\bar{X} = \\frac{1}{n}\\sum_{i=1}^n x_i \\\\\n\\text{Sample Variance: } \\ \\ &  S^2 = \\frac{1}{n-1}\\sum_{i=1}^n ( x_i - \\bar{X}  )^2 \\\\\n\\text{Sample Standard Deviation:} \\ \\ & s = \\sqrt{\\frac{1}{n-1}\\sum_{i=1}^n ( x_i - \\bar{X} )^2}\n\\end{align}\n\\]\nNote: the standard deviation is the square root of the variance\n\n\n\nTwo Variables – Visualizations and Summary Statistics\n\nScatter Plots\n\n\n\n\n\n\n\n\n\nA scatterplot is the main tool to visualize and identify a relationship between two quantitative variables. Oftentimes, coloring each observation by another categorical variable is a way to maximize effectiveness of a single plot, as you are encoding more information within the same space.\n\n\nBar Plots\n\n\n\n\n\n\n\n\n\nBar charts are visualizations when focusing your audience on a single categorical and a summary statistic or aspect of a quantitative variable. In practice, scatter-plots show more information since you can encode the categorical variable through another channel such as color, size, shape while keeping the horizontal and vertical axis for two separate quantitative variables.\n\n\nCorrelation\n\n\n\n\n\n\ncarat\ndepth\ntable\nprice\n\n\n\n\ncarat\n1.00\n0.00\n0.22\n0.91\n\n\ndepth\n0.00\n1.00\n-0.32\n-0.02\n\n\ntable\n0.22\n-0.32\n1.00\n0.17\n\n\nprice\n0.91\n-0.02\n0.17\n1.00\n\n\n\n\n\nCorrelation is the only multivariate summary statistic we will be using in this course, used to describe how two variables tend to move in tandem with one another. A perfect linear association evokes a correlation of 1, the opposite being a perfect negative association with a correlation of -1. No association is implied by a correlation near 0.\nMathematically: &gt; Definition &gt; For any two variables X,Y, the correlation of X and Y are: \\[\nr = \\frac{\\sum_{i=1}^n (x_i - \\bar{x})(y_i-\\bar{y})}{\\sum_{i=1}^n (x_i - \\bar{x})^2\\sum_{i=1}^n (y_i - \\bar{y})^2}\n\\]"
  },
  {
    "objectID": "ma206_course.html#probability",
    "href": "ma206_course.html#probability",
    "title": "MA206: Introduction to Probability and Statistics",
    "section": "Probability",
    "text": "Probability\n\nSample Space and Random Experiment (\\(\\Omega\\))\nA random experiment is a process that produces an outcome which cannot be predicted with certainty in advance. It must be well-defined, have more than one possible outcome, and be repeatable under similar conditions. Each performance of the experiment results in a single outcome from the sample space. The sample space is the set of all possible outcomes of a random experiment.\n\n\nDefinition:\n\nThe sample space is the set of all possible outcomes of a random experiment, denoted \\(\\Omega\\).\nAn event is a subset of the sample space. It can represent one or more outcomes.\nIf all outcomes in \\(\\Omega\\) are equally likely, then for any event \\(A\\):\n\n\n\\[\n\\mathbb{P}(A) = \\frac{\\text{Number of outcomes in } A}{\\text{Total outcomes in } \\Omega}\n\\]\n\nExamples:\n\nTossing a coin once: \\(\\Omega = {\\text{Heads}, \\text{Tails}}\\)\nRolling a 6-sided die: \\(\\Omega = {1, 2, 3, 4, 5, 6}\\)\nLetter grade in MA206: \\(\\Omega = {A, B, C, D, F}\\)\nNumber of emails received in an hour: \\(\\Omega = {0, 1, 2, \\dots}\\)\n\n\n\n\nProbability Measure (\\(\\mathbb{P}\\))\nA probability measure is a rule, denoted \\(\\mathbb{P}\\), that assigns a number between 0 and 1 to every event in a collection of events (called a sigma-algebra, denoted \\(\\mathcal{F}\\) ). These probabilities must follow three key rules, known as the axioms of probability.\n\n\n\n\n\n\n\nNoteAxioms of Probability\n\n\n\n\nNon-Negativity\nFor any event \\(A\\), the probability is never negative:\n\\[\n\\mathbb{P}(A) \\geq 0\n\\]\nNormalization\nThe probability of one of the events happening over the entire sample space is 1:\n\\[\n\\mathbb{P}(\\Omega) = 1\n\\]\nAdditivity (for disjoint events)\nIf events \\(A_1, A_2, A_3, \\dots\\) are mutually exclusive (no overlap), then the probability that any one of them occurs is the sum of their individual probabilities:\n\\[\n\\mathbb{P}\\left(\\bigcup_{i=1}^{\\infty} A_i\\right) = \\mathbb{P}(A_1) + \\mathbb{P}(A_2) + \\mathbb{P}(A_3) + \\cdots\n\\]\n\n\n\n\nExample (Simple):\nLet \\(A\\), \\(B\\), and \\(C\\) be outcomes when rolling a die:\n\n\\(A = \\{1\\}\\), $B = {3} $, \\(C = \\{5\\}\\)\n\nThese are disjoint events (they don’t overlap).\n\nThen: \\[\n\\mathbb{P}(A \\cup B \\cup C) = \\mathbb{P}(A) + \\mathbb{P}(B) + \\mathbb{P}(C) = \\frac{1}{6} + \\frac{1}{6} + \\frac{1}{6} = \\frac{1}{2}\n\\]\n\nThese three rules form the mathematical foundation of all probability calculations — everything else builds on them.\n\n\n\n\n\n\n\nNoteSet Theory\n\n\n\n\nThe complement of an event \\(A\\), written \\(A^c\\), consists of all outcomes in \\(\\Omega\\) that are not in \\(A\\):\n\n\\[\n\\mathbb{P}(A) + \\mathbb{P}(A^c) = 1\n\\]\n\nThe intersection \\(A \\cap B\\) consists of outcomes where both \\(A\\) and \\(B\\) occur.\nThe union \\(A \\cup B\\) consists of outcomes where either \\(A\\), \\(B\\), or both occur:\n\n\\[\n\\mathbb{P}(A \\cup B) = \\mathbb{P}(A) + \\mathbb{P}(B) - \\mathbb{P}(A \\cap B)\n\\]\n\nTwo events \\(A\\) and \\(B\\) are disjoint (mutually exclusive) if they cannot both occur:\n\n\\[\nA \\cap B = \\varnothing \\quad \\text{and} \\quad \\mathbb{P}(A \\cap B) = 0\n\\]\n\n\n\n\nConditional Probability\nFor events \\(A\\) and \\(B\\) with \\(0 &lt; P(B) \\le 1\\), the conditional probability of \\(A\\) given \\(B\\) is:\n\\[\nP(A \\mid B) = \\frac{P(A \\cap B)}{P(B)}\n\\]\nExample: One card is drawn from a standard deck.\nLet \\(A\\): card is a Queen, and \\(B\\): card is a face card.\nFind \\(P(A)\\), \\(P(B)\\), and \\(P(A \\mid B)\\).\n\n\n\nLaw of Total Probability\n\n\n\n\n\n\nNoteLoTP\n\n\n\nIf \\(E_1, \\dots, E_n\\) is a partition of the sample space (mutually exclusive and exhaustive), then for any event \\(A\\):\n\\[\nP(A) = \\sum_{i=1}^{n} P(E_i) P(A \\mid E_i)\n\\]\n\n\nExample:\nA fair die is rolled. Let event A: “an even number is rolled”.\nLet:\n- \\(E_1\\): roll is 1 or 2\n- \\(E_2\\): roll is 3 or 4\n- \\(E_3\\): roll is 5 or 6\n\nFind:\n- \\(P(A \\mid E_1)\\), \\(P(A \\mid E_2)\\), \\(P(A \\mid E_3)\\)\n- Then use the Law of Total Probability to find \\(P(A)\\)\n\n\n\nBayes’ Theorem\n\n\n\n\n\n\nNoteSwitching the Conditioning of Events\n\n\n\n\nDefinition:\nIf \\(E_1, \\dots, E_n\\) is a partition of the sample space and \\(P(A) &gt; 0\\), then:\n\n\\[\nP(E_k \\mid A) = \\frac{P(A \\mid E_k) P(E_k)}{\\sum_{i=1}^n P(A \\mid E_i) P(E_i)}\n\\]\n\n\nExample:\nTwo urns:\n- Urn 1: 1 red, 1 blue\n- Urn 2: 3 red, 1 blue\nPick an urn at random, then draw one ball.\nIf the ball is red, what is the probability it came from Urn 1?\n\n\n\nCounting Principles\nBefore we can begin a thorough treatment of probability, some concepts in counting are needed to identify four common situations. These arise depending on when things are “allowed” to repeat or the “order” items are chosen in matters. The ability to discern when these four situations arise is more than half the battle.\n\nOrdered with Replacement\nThink of the number of ways of choosing a 4-digit passcode on your phone.\nThe order of the numbers matters, and you are allowed to repeat the same number. So how many arrangements are there? Since repetition is allowed and order matters, there are 10 digits for each position, giving:\n\\[\n\\text{Ordered Arrangements with Replacement} = n^r = 10^4 = 10{,}000\n\\]\n\n\nOrdered without Replacement\nThink of the number of ways I can create a batting order from 9 position players.\nThe order still matters, but players cannot be repeated. This is a permutation — an ordered arrangement without replacement.\n\\[\n{}_nP_r = P(n, r) = \\frac{n!}{(n - r)!}\n\\]\nNote: Factorial in math is computed as \\(5! = 5 \\times 4 \\times 3 \\times 2 \\times 1 = 120\\).\nFor example, the number of ways to assign the first 3 batting positions from 9 players:\n\\[\n{}_9P_3 = \\frac{9!}{(9-3)!} = 9 \\times 8 \\times 7 = 504\n\\]\n\n\nUnordered without Replacement\nThink of how many ways you can choose 3 scoops of ice cream from 5 unique flavors without repeats.\nBecause order doesn’t matter and repeats aren’t allowed, we use combinations:\n\\[\n{}_nC_k = \\binom{n}{k} = \\frac{n!}{k!(n-k)!}\n\\]\n\n\nUnordered with Replacement\nThink of how many different combinations of 3 scoop ice cream cones you can make with 5 unique flavors while allowing repeats.\nThis is the trickiest scenario. The order doesn’t matter, and repetition is allowed. The formula is:\n\\[\n\\text{Unordered Arrangements with Replacement} = \\binom{r+n-1}{r} = \\frac{(r+n-1)!}{r!(n-1)!}\n\\]\nExample: choosing 3 scoops from 5 flavors (with repeats):\n\\[\n\\binom{3+5-1}{3} = \\binom{7}{3} = 35\n\\]\nThis can be understood using the stars and bars method: selecting \\(r\\) scoops with \\(n-1\\) dividers. Imagine representing each scoop as a ★ (star) and using vertical bars | to separate flavor types. To choose \\(r\\) scoops from \\(n\\) flavors, you need \\(r\\) stars (for the scoops) and \\(n - 1\\) bars (to divide them into \\(n\\) categories). For example, if \\(r = 3\\) scoops and \\(n = 5\\) flavors, you arrange 3 stars and 4 bars in a row. One possible arrangement is ★ | ★★ | | |, which represents 1 scoop of flavor 1, 2 scoops of flavor 2, and 0 scoops of flavors 3, 4, and 5. The number of such arrangements is given by the combination formula \\(\\binom{r + n - 1}{r}\\), since you are choosing positions for the \\(r\\) indistinguishable stars among the \\(r + n - 1\\) total positions (stars and bars combined).\nNote: The above section may seem like it came out of nowhere, that is okay. A fundamental difficulty in probability is finding the sample space or event space due to finding the various different combinations/permutations sequences of different possibilities. To elaborate consider the next example:\n\nExample: No Matching Pairs in a Shoe Sample\nA closet contains \\(n\\) pairs of shoes (so \\(2n\\) total shoes). If \\(2r\\) shoes are chosen at random (where \\(2r &lt; n\\)), what is the probability that no matching pair is selected?\nWe are selecting \\(2r\\) shoes such that no left and right shoe from the same pair are both chosen.\nStrategy:\n1. First choose \\(2r\\) distinct pairs from the \\(n\\) available — there are \\(\\binom{n}{2r}\\) ways to do this.\n2. From each of these \\(2r\\) selected pairs, choose only one shoe (either left or right) — there are \\(2^{2r}\\) ways to do this.\n3. The total number of ways to choose any \\(2r\\) shoes out of \\(2n\\) is \\(\\binom{2n}{2r}\\).\nSo, the desired probability is:\n\\[\n\\mathbb{P}(\\text{No matching pair}) = \\frac{\\binom{n}{2r} \\cdot 2^{2r}}{\\binom{2n}{2r}}\n\\]"
  },
  {
    "objectID": "ma206_course.html#random-variables-expectation-and-variance",
    "href": "ma206_course.html#random-variables-expectation-and-variance",
    "title": "MA206: Introduction to Probability and Statistics",
    "section": "Random Variables, Expectation, and Variance",
    "text": "Random Variables, Expectation, and Variance\n\n\n\n\n\n\nNoteRandom Variable\n\n\n\nA random variable is a mapping that assigns a real number to every outcome in the sample space:\n\\[\nX: \\Omega \\rightarrow \\mathbb{R}\n\\]\n\n\n\n\n\n\n\n\nNoteCumulative Distribution Function\n\n\n\nA cumulative distribution function (CDF) is a function \\(F_X: \\mathbb{R} \\rightarrow [0,1]\\) defined by:\n\\[\nF_X(x) = \\mathbb{P}(X \\le x)\n\\]"
  },
  {
    "objectID": "ma206_course.html#discrete-random-variables",
    "href": "ma206_course.html#discrete-random-variables",
    "title": "MA206: Introduction to Probability and Statistics",
    "section": "Discrete Random Variables",
    "text": "Discrete Random Variables\n\n\n\n\n\n\nNoteDiscrete Random Variables\n\n\n\nA discrete random variable is a random variable that takes countably many values in \\(\\mathbb{R}\\).\nIts probability mass function is given by:\n\\[\nf_X(x) = \\mathbb{P}(X = x)\n\\]\nThe expected value (mean) of a discrete random variable \\(X\\) is:\n\\[\n\\mathbb{E}[X] = \\sum_x x \\cdot \\mathbb{P}(X = x)\n\\]\nDefinition (Variance):\nThe variance of a discrete random variable \\(X\\) is:\n\\[\n\\mathrm{Var}(X) = \\mathbb{E}\\left[(X - \\mathbb{E}[X])^2\\right] = \\sum_x (x - \\mathbb{E}[X])^2 \\cdot \\mathbb{P}(X = x)\n\\]\n\n\n\nBinomial Distribution:\n\nLet \\(X \\sim \\text{Binomial}(n, p)\\) where \\(n \\in \\mathbb{N}\\) and \\(0 &lt; p &lt; 1\\).\n\nProbability Mass Function (PMF):\n\\[\n\\mathbb{P}(X = k) = \\binom{n}{k} p^k (1 - p)^{n - k}, \\quad \\text{for } k = 0, 1, \\dots, n\n\\]\nCumulative Distribution Function (CDF):\n\\[\nF_X(x) = \\mathbb{P}(X \\le x) = \\sum_{k=0}^{\\lfloor x \\rfloor} \\binom{n}{k} p^k (1 - p)^{n - k}\n\\]\nExpected Value:\n\\[\n\\mathbb{E}[X] = np\n\\]\nVariance:\n\\[\n\\mathrm{Var}(X) = np(1 - p)\n\\]\n\n\n\n\nGeometric Distribution:\n\nLet \\(X \\sim \\text{Geometric}(p)\\) be the number of trials until the first success (including the success), where \\(0 &lt; p &lt; 1\\).\n\nProbability Mass Function (PMF):\n\\[\n\\mathbb{P}(X = k) = (1 - p)^{k - 1} p, \\quad \\text{for } k = 1, 2, 3, \\dots\n\\]\nCumulative Distribution Function (CDF):\n\\[\nF_X(x) = \\mathbb{P}(X \\le x) = 1 - (1 - p)^{\\lfloor x \\rfloor}\n\\]\nExpected Value:\n\\[\n\\mathbb{E}[X] = \\frac{1}{p}\n\\]\nVariance:\n\\[\n\\mathrm{Var}(X) = \\frac{1 - p}{p^2}\n\\]"
  },
  {
    "objectID": "ma206_course.html#continuous-random-variables",
    "href": "ma206_course.html#continuous-random-variables",
    "title": "MA206: Introduction to Probability and Statistics",
    "section": "Continuous Random Variables",
    "text": "Continuous Random Variables\n\n\n\n\n\n\nNoteContinuous Random Variable\n\n\n\nA continuous random variable takes infinitely many values in \\(\\mathbb{R}\\). Its probability density function is given by:\n\\[\nf_X(x) = \\mathbb{P}(X = x)\n\\]\nNote: We do not find probabilities with the pdf like the pmf of a discrete random variable. We integrate over a neighborhood of the support of X, as there is no probability mass at any single point for a continuous distribution.\nThe expected value (mean) of a continuous random variable \\(X\\) is:\n\\[\n\\mathbb{E}[X] = \\int_{-\\infty}^\\infty x f_X(x)dx\n\\]\nDefinition (Variance):\nThe variance of a discrete random variable \\(X\\) is:\n\\[\n\\mathrm{Var}(X) = \\mathbb{E}\\left[(X - \\mathbb{E}[X])^2\\right] = \\int_{-\\infty}^\\infty (x - \\mathbb{E}[X])^2 f_X(x)dx = \\mathbb{E}[X^2] - \\mathbb{E}[X]^2\n\\]\n\n\n\nCentral Limit Theorem (CLT)\nThe Central Limit Theorem (CLT) is one of the most important results in statistics. It states that the sampling distribution of the sample mean \\(\\bar{X}\\) becomes approximately normal as the sample size \\(n\\) increases, regardless of the shape of the population distribution (provided it has finite mean and variance).\nSpecifically, if \\(X_1, X_2, \\dots, X_n\\) are i.i.d. random variables with mean \\(\\mu\\) and variance \\(\\sigma^2\\), then:\n\\[\n\\frac{\\bar{X} - \\mu}{\\sigma / \\sqrt{n}} \\xrightarrow{d} \\mathcal{N}(0,1) \\quad \\text{as } n \\to \\infty\n\\]\nThis justifies the widespread use of the normal distribution to approximate sample means in practice.\n\n\n\n\nCredit: The New York Times\n\n\n\nNormal Distribution\nThe above video showed you the importance of this distribution, also called a Gaussian Distribution. Many natural phenomena are normally distributed.\n\nNormal Distribution\nLet \\(X \\sim \\mathcal{N}(\\mu, \\sigma^2)\\), where \\(\\mu \\in \\mathbb{R}\\) and \\(\\sigma &gt; 0\\).\n\nProbability Density Function (PDF):\n\\[\nf(x) = \\frac{1}{\\sqrt{2\\pi \\sigma^2}} \\exp\\left( -\\frac{(x - \\mu)^2}{2\\sigma^2} \\right), \\quad x \\in \\mathbb{R}\n\\]\nCumulative Distribution Function (CDF):\nThere is no closed-form expression, but it is denoted as:\n\\[\nF_X(x) = \\mathbb{P}(X \\le x) = \\Phi\\left( \\frac{x - \\mu}{\\sigma} \\right)\n\\]\nwhere \\(\\Phi\\) is the standard normal CDF.\nExpected Value:\n\\[\n\\mathbb{E}[X] = \\mu\n\\]\nVariance:\n\\[\n\\mathrm{Var}(X) = \\sigma^2\n\\]\n\n\n\n\nExponential Distribution\nThis distribution is helpful to model continuous time-related events: time between system failures at a factory, time between phone calls at a call center, time between insurance claims received at a insurance firm. All these can be modeled with this useful continuous random variable.\n\nLet \\(X \\sim \\text{Exponential}(\\lambda)\\) with \\(\\lambda &gt; 0\\).\n\nProbability Density Function (PDF):\n\\[\nf(x) = \\lambda e^{-\\lambda x}, \\quad x \\ge 0\n\\]\nCumulative Distribution Function (CDF):\n\\[\nF_X(x) = \\mathbb{P}(X \\le x) = 1 - e^{-\\lambda x}, \\quad x \\ge 0\n\\]\nExpected Value:\n\\[\n\\mathbb{E}[X] = \\frac{1}{\\lambda}\n\\]\nVariance:\n\\[\n\\mathrm{Var}(X) = \\frac{1}{\\lambda^2}\n\\]"
  },
  {
    "objectID": "ma206_course.html#confidence-intervals",
    "href": "ma206_course.html#confidence-intervals",
    "title": "MA206: Introduction to Probability and Statistics",
    "section": "Confidence Intervals",
    "text": "Confidence Intervals\nA confidence interval (CI) gives a range of plausible values for a population parameter based on a sample statistic. The general structure of any confidence interval is:\n\\[\n\\text{point estimate} \\ \\pm \\ \\text{margin of error}\n\\]\nMore specifically, for large samples or when the sampling distribution of the estimate is approximately normal:\n\\[\n\\text{CI} = \\hat{\\theta} \\ \\pm \\ z^* \\cdot \\text{SE}(\\hat{\\theta})\n\\]\nWhere:\n\n\\(\\hat{\\theta}\\) is the point estimate (e.g., \\(\\bar{x}\\) for the mean, \\(\\hat{p}\\) for a proportion)\n\\(z^*\\) is the critical value from the standard normal distribution (e.g., 1.96 for 95% confidence)\n\\(\\text{SE}(\\hat{\\theta})\\) is the standard error of the estimate\n\nThis structure applies to many common settings:\n\nCI for a population mean: \\[\n\\bar{x} \\pm z^* \\cdot \\frac{s}{\\sqrt{n}}\n\\]\nCI for a population proportion: \\[\n\\hat{p} \\pm z^* \\cdot \\sqrt{\\frac{\\hat{p}(1 - \\hat{p})}{n}}\n\\]\n\n\nInterpretation:\n\n“We are 95% confident that the true population parameter lies within this interval.”\n\nThis does not mean there’s a 95% probability the parameter is in the interval — rather, it means that 95% of all intervals computed from repeated samples in this manner would contain the true parameter."
  },
  {
    "objectID": "ma206_course.html#one-sample-hypothesis-testing",
    "href": "ma206_course.html#one-sample-hypothesis-testing",
    "title": "MA206: Introduction to Probability and Statistics",
    "section": "One Sample Hypothesis Testing",
    "text": "One Sample Hypothesis Testing\nHypothesis testing is a formal method for making inferences about a population using sample data. The whole test aspect is questioning if the statistic your sample shows is significantly different than a certain value in question. You have two underlying premises, referred to as the null and alternative hypotheses. The null hypothesis assumes that there is no difference: the statistic from your value is the same as the value you are testing. The alternative conflicts the null and says they are different. The process involves:\n\nState the null and alternative hypotheses. There are three different variants to create your hypotheses statements depending on what the question being asked entails.\n\n\n\nGreater than Alternative Hypothesis\n\n\\[\n\\begin{align}\nH_0: Parameter &= Value \\ in \\ Question \\\\\nH_A: Parameter &&gt; Value \\ in \\ Question\n\\end{align}\n\\]\nThe entire inference aspect of hypothesis testing is that you are using your sample statistic, a tangible aspect of your data, to make an argument about the population parameter, an entity that is unknown to you. This is why the hypotheses are written in terms of the parameter. You are testing whether an aspect or parameter about the population is greater than a benchmark value decided by you in advance.\n\nLess than Alternative Hypothesis\n\n\\[\n\\begin{align}\nH_0: Parameter &= Value \\ in \\ Question \\\\\nH_A: Parameter &&lt; Value \\ in \\ Question\n\\end{align}\n\\]\nVery similarly, the less than hypothesis is also a one-sided hypothesis test in that you are only testing one side of the value, abeit this time if the population parameter is less than the tested value.\n\nNot equal to Alternative Hypothesis\n\n\\[\n\\begin{align}\nH_0: Parameter &= Value \\ in \\ Question \\\\\nH_A: Parameter &\\neq Value \\ in \\ Question\n\\end{align}\n\\]\nThis is the only two-sided hypothesis test, denoted with the not equal to alternative hypothesis. Both sides must be accounted for in this test, and therefore as we will see shortly require more evidence for significance.\n\nChoose a significance level \\(\\alpha\\).\n\nThis is the threshold you will also decide to inform your certainty in your conclusions. As seen previously, unless you are finding the probability that something will happen in the entire sample space (which happens probability 1); ie, there are no absolutes. Therefore there is always a chance that your conclusion will be wrong. Here is where you decide how “often” you are willing to be wrong. Is it 1% of the time? 5% of the time? Think of the significance level as choosing the percentage of the time you are willing to be wrong in your conclusion, (I know this sounds weird). A common \\(\\alpha\\) is 5%.\n\nCompute the test statistic.\n\nCompute the relevant summary statistic. In this course you will either be calculating a sample proportion, denoted \\(\\hat{p}\\) if the variable of interest is categorical or the sample mean \\(\\bar{X}\\) if quantitative.\n\nStandardize the test statistic.\n\nThis step transforms your statistic so it can be treated as a random variable from a named distribution. For proportions this will be a Normal Random Variable, however if quantitative it will be a t-distributed random variable.\n\n\n\n\nCredit: 365 Data Science\n\nDetermine the p-value.\n\nOnce we have the standardized statistic, it can be treated as either a Standard Normal Random Variable or Student’s-t Random Variable. This is where the alternative hypotheses come in to play to determine the p-value: the probability that if the null hypothesis is indeed true you choose to make an argument supporting the alternative. To find the probability of a certain event happening in a continuous random variable, you integrate the probability density function with the limits of integration being the range of values the random variable could take. Both the Standard Normal and Student’s-t are continuous, so depending on your alternative hypothesis, your p-value is calculated by either of the following:\n\\[\n\\begin{align}\n\\text{Greater Hypothesis} &&&& \\text{Less than Hypothesis} &&&& \\text{Two Sided Hypothesis} \\\\\n\\mathbb{P}(X&gt;z) = \\int_z^\\infty f_X(x) dx &&&& \\mathbb{P}(X&lt;z) = \\int_{-\\infty}^z f_X(x)dx &&&& \\mathbb{P}(X&gt;z) = \\int_{|z|}^\\infty f_X(x)dx + \\int_{-\\infty}^{-|z|} f_X(x)dx\n\\end{align}\n\\]\n\n\nIgnoring unknown labels:\n• parse : \"TRUE\"\n\n\n\n\n\n\n\n\n\n\nMaking a conclusion based on comparison.\n\nOnce a p-value is obtained, reference it to the significance level chosen. If the p-value is greater than \\(\\alpha\\), you fail to reject the null hypothesis, if it is smaller, you reject the null hypothesis, and then state what that means in the context of the problem.\n\n\nSingle Proportion\nWe conduct inference on a population proportion \\(\\pi\\) relative to a hypothesized value \\(\\pi_0\\). The test statistic is:\n\\[\nz = \\frac{\\hat{p} - \\pi}{\\sqrt{\\frac{\\pi(1 - \\pi)}{n}}}\n\\]\n\nExample: Test if more than 40% of diamonds are “Ideal” cut\n\n# Null hypothesis: pi = 0.40\npi &lt;- 0.4\nn &lt;- nrow(diamonds)\nphat &lt;- mean(diamonds$cut == \"Ideal\")\n\n# Test statistic and p-value\nz &lt;- (phat - pi) / sqrt(pi * (1 - pi) / n)\np_value &lt;- 1 - pnorm(z)\n\ncat(\"Z-statistic:\", round(z, 3), \"\\n\")\n\nZ-statistic: -0.22 \n\ncat(\"P-value:\", round(p_value, 4))\n\nP-value: 0.587\n\n\n\n\n\n\nSingle Mean\nWe conduct inference on a population mean \\(\\mu\\) relative to a hypothesized value \\(\\mu_0\\). The test statistic is:\n\\[\nt = \\frac{\\bar{x} - \\mu_0}{s / \\sqrt{n}}\n\\]\n\nExample: Test if the average diamond price is $4000.\n\nmu0 &lt;- 4000\nx_bar &lt;- mean(diamonds$price)\ns &lt;- sd(diamonds$price)\nn &lt;- length(diamonds$price)\n\n# Test statistic and p-value\nt_stat &lt;- (x_bar - mu0) / (s / sqrt(n))\np_value &lt;- 2*(1 - pt(abs(t_stat), df = n - 1))\n\ncat(\"T-statistic:\", round(t_stat, 3), \"\\n\")\n\nT-statistic: -3.912 \n\ncat(\"P-value:\", round(p_value, 4))\n\nP-value: 1e-04\n\n\n\n\n\nExperimental Design: Power, Type I / Type II Error\nIn any hypothesis test, we face the possibility of making incorrect conclusions. These are formalized through Type I and Type II errors:\n\nType I Error (\\(\\alpha\\)): Rejecting the null hypothesis when it is actually true. This is controlled by the significance level of the test, often set to \\(\\alpha = 0.05\\).\nType II Error (\\(\\beta\\)): Failing to reject the null hypothesis when the alternative is actually true. This is harder to control and depends on the true parameter, sample size, and variance.\nPower of the Test: The probability of correctly rejecting the null hypothesis when the alternative is true: \\[\n\\text{Power} = 1 - \\beta\n\\]\n\nA powerful test detects meaningful effects and minimizes Type II error. Power increases when: - Sample size increases (\\(n \\uparrow\\)) - Effect size increases (true parameter is farther from null) - Variability decreases (standard deviation \\(\\downarrow\\)) - Significance level \\(\\alpha\\) increases (easier to reject null)\n\n\n\n\n\n\n\n\n\n\n\nThis diagram shows: - The blue curve is the null distribution (centered at 0). - The red dashed curve is the alternative distribution (shifted mean). - The dotted vertical line is the critical value (e.g., \\(z = 1.645\\) for \\(\\alpha = 0.05\\) in a one-sided test). - Area to the right of this cutoff under the null curve is \\(\\alpha\\). - Area to the left of this cutoff under the alternative curve is \\(\\beta\\). - The remaining area under the red curve (right tail) is power."
  },
  {
    "objectID": "ma206_course.html#two-sample-hypothesis-testing",
    "href": "ma206_course.html#two-sample-hypothesis-testing",
    "title": "MA206: Introduction to Probability and Statistics",
    "section": "Two Sample Hypothesis Testing",
    "text": "Two Sample Hypothesis Testing\n\nDifference of Proportions\nWe compare two population proportions to determine whether there is a significant difference between them. The test statistic is:\n\\[\nz = \\frac{\\hat{p}_1 - \\hat{p}_2}{\\sqrt{\\hat{p}(1 - \\hat{p}) \\left( \\frac{1}{n_1} + \\frac{1}{n_2} \\right)}}\n\\]\nWhere \\(\\hat{p}\\) is the pooled proportion.\n\nExample: Are “Ideal” cuts more common among Premium vs. Good color grades?\n\n# Subset to just Premium and Good clarity levels\ndf &lt;- diamonds %&gt;% filter(color ==\"D\" | color==\"E\")\n\ntable(df$cut, df$color)\n\n           \n               D    E    F    G    H    I    J\n  Fair       163  224    0    0    0    0    0\n  Good       662  933    0    0    0    0    0\n  Very Good 1513 2400    0    0    0    0    0\n  Premium   1603 2337    0    0    0    0    0\n  Ideal     2834 3903    0    0    0    0    0\n\n# Create a binary outcome: Ideal or not\ndf &lt;- df %&gt;%\n  mutate(is_ideal = cut == \"Ideal\")\n\n# Proportions\np1 &lt;- mean(df$is_ideal[df$color == \"D\"])\np2 &lt;- mean(df$is_ideal[df$color == \"E\"])\nn1 &lt;- sum(df$color == \"D\")\nn2 &lt;- sum(df$color == \"E\")\n\n# Pooled proportion\nphat &lt;- (p1 * n1 + p2 * n2) / (n1 + n2)\n\n# Test statistic\nz &lt;- (p1 - p2) / sqrt(phat * (1 - phat) * (1 / n1 + 1 / n2))\np_value &lt;- 2 * (1 - pnorm(abs(z)))\n\ncat(\"Z-statistic:\", round(z, 3), \"\\n\")\n\nZ-statistic: 2.566 \n\ncat(\"P-value:\", round(p_value, 4))\n\nP-value: 0.0103\n\n\n\n\n\n\nMultiple Proportions (Chi-Square Test of Independence)\nUsed when comparing proportions across more than two groups.\n\nExample: Is cut independent of color?\n\ntbl &lt;- table(diamonds$cut, diamonds$color)\nchisq.test(tbl)\n\n\n    Pearson's Chi-squared test\n\ndata:  tbl\nX-squared = 310.32, df = 24, p-value &lt; 2.2e-16\n\n\nThis test checks whether the distribution of cut types is independent of the diamond color. A small p-value suggests a dependency.\n\n\n\n\nDifference of Means\nUsed to compare two independent sample means.\n\nExample: Is the average price different between “Ideal” and “Fair” cuts?\n\ndf &lt;- diamonds %&gt;% filter(cut %in% c(\"Ideal\", \"Fair\"))\n\nt.test(price ~ cut, data = df)\n\n\n    Welch Two Sample t-test\n\ndata:  price by cut\nt = 9.7484, df = 1894.8, p-value &lt; 2.2e-16\nalternative hypothesis: true difference in means between group Fair and group Ideal is not equal to 0\n95 percent confidence interval:\n  719.9065 1082.5251\nsample estimates:\n mean in group Fair mean in group Ideal \n           4358.758            3457.542 \n\n\nThis performs a two-sample t-test, assuming unequal variances by default. The null hypothesis is that the means are equal.\n\n\n\n\nPaired Data\nIn paired designs, each observation in one group is paired with a related observation in the other. Since diamonds has no natural pairing, we’ll simulate a paired example.\n\nExample (Simulated): Price before and after resizing a set of diamonds\n\nset.seed(123)\n\n# Simulate paired prices: original and discounted\nn &lt;- 100\noriginal_price &lt;- sample(diamonds$price, n)\ndiscounted_price &lt;- original_price * runif(n, 0.85, 0.95)\n\nt.test(original_price, discounted_price, paired = TRUE)\n\n\n    Paired t-test\n\ndata:  original_price and discounted_price\nt = 8.9496, df = 99, p-value = 2.132e-14\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n 342.2012 537.1645\nsample estimates:\nmean difference \n       439.6829 \n\n\nThis tests whether the mean price before and after a simulated discount differs significantly."
  },
  {
    "objectID": "ma206_course.html#regression",
    "href": "ma206_course.html#regression",
    "title": "MA206: Introduction to Probability and Statistics",
    "section": "Regression",
    "text": "Regression\nIn this section we continue our multivariate inference with creating models for the purpose of identifying significance between explanatory (predictor) variables and the response. The function used to create the model, \\(\\hat{y_i}=f(x_i)\\) will make predictions, known as fitted values. Do to the variability in the data, these fitted values will not exactly predict the response (ie. \\(y_i \\neq \\hat{y}\\)) for all values in the response. These errors are the deviations from the response and the fitted values and are referred as residuals, with notation \\(\\epsilon_i = y_i - \\hat{y}_i\\).\nTo assess how well a model performs, the residuals are summarized in a few different methods:\n\nMean Absolute Deviation\nThe average magnitude of the residuals:\n\\[\nMAD = \\frac{1}{n}\\sum_{i=1}^n |y_i - \\hat{y}_i|\n\\]\n\n\nMean Squared Error:\nThe average magnitude of the residual-squared.\n\\[\nMSE = \\frac{1}{n}\\sum_{i=1}^n (y_i - \\hat{y}_i)^2\n\\]\n\n\nSimple Linear Regression\nThe above metrics could be applied to any model, however the central method to assess a linear relationship between two quantitative variables isSimple Linear Regression, or better known as the line of best fit:\n\\[\n\\hat{y} = \\beta_0 + \\beta_1 x\n\\]\nThe \\(\\beta\\)’s are the parameters of the model: the y-intercept and slope. The reason the method is the best fit is because we optimizes the choices for these two parameters by minimizing the sum of squared error:\n\\[\n\\begin{align}\nSSE &= \\sum_{i=1}^n (\\epsilon_i)^2 \\\\\n    &= \\sum_{i=1}^n (y_i - \\widehat{y}_i)^2 \\\\\n    &= \\sum_{i=1}^n (y_i - \\beta_0 - \\beta_1 x_i)^2 \\\\\n\\frac{\\partial}{\\partial \\beta_0}SSE &= \\frac{\\partial}{\\partial \\beta_0} \\sum_{i=1}^n (y_i - \\beta_0 - \\beta_1x_i)^2 \\\\\n0 &= -2\\sum_{i=1}^n (y_i - \\beta_0 - \\beta_1x_i) \\\\\n\\widehat{\\beta}_0 &= \\bar{y} - \\widehat{\\beta}_1 \\bar{x} \\\\\n\\frac{\\partial}{\\partial \\beta_1}SSE &= \\frac{\\partial}{\\partial \\beta_1} \\sum_{i=1}^n (y_i - \\beta_0 - \\beta_1x_i)^2 \\\\\n&= \\frac{\\partial}{\\partial \\beta_1} \\sum_{i=1}^n (y_i - (\\bar{y} - \\widehat{\\beta}_1 \\bar{x}) - \\widehat{\\beta}_1x_i)^2 \\\\\n0 &= -2\\sum_{i=1}^n (\\bar{x} - x_i)(y_i - \\bar{y} + \\widehat{\\beta}_1( \\bar{x} - x_i)) \\\\\n\\widehat{\\beta}_1 \\sum_{i=1}^n (x_i - \\bar{x})^2 &= \\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y}) \\\\\n\\widehat{\\beta}_1 &= \\frac{\\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^n (x_i - \\bar{x})^2} = \\frac{\\mathrm{Cov}(X,Y)}{\\mathrm{Var}(X)}\n\\end{align}\n\\]\nThe above is beyond the scope of this course, however it warrants a healthy appreciation for finding the line of best fit!\nIn practice, from the diamonds dataset we could model price as a function of carat:\n\n# Sample and fit model\nset.seed(206)\ndf &lt;- ggplot2::diamonds %&gt;% sample_n(1000)\nlm_simple &lt;- lm(price ~ carat, data = df)\nsummary(lm_simple)\n\n\nCall:\nlm(formula = price ~ carat, data = df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8077.3  -813.1    10.2   607.5  8759.7 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -2376.64      93.40  -25.45   &lt;2e-16 ***\ncarat        7885.65      99.37   79.35   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1499 on 998 degrees of freedom\nMultiple R-squared:  0.8632,    Adjusted R-squared:  0.8631 \nF-statistic:  6297 on 1 and 998 DF,  p-value: &lt; 2.2e-16\n\n\nThere is a lot in the summary output, however the main ideas here lie in the magnitude and sign of the coefficient, looking for practical significance, and also looking at the size of the p-value relative to a chosen \\(\\alpha\\), checking for statistical significance.\nYou may be wondering in a line of best fit, where did the p-value come from? Good question! In addition to finding the line of best fit, our linear model assesses the relevance of all parameters in the model. This assessment is a *one-sample t-test for every ! If there is significance, then there is a significant assocation between the explanatory variable and the response.\n\n\n\n\n\n\n\n\n\n\n\n\nMultiple Linear Regression\nWe can extend linear regression to in fact include as many predictor variables as we want (as long as we have more observations than variables!). This is implemented through Multiple Linear Regression:\n\\[\n\\widehat{y} = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\dots + \\beta_p x_p\n\\]\nThe derivation would be too lengthy to do them individually previously, however through Matrix Algebra (MA371 anyone?!), the solved vector of coefficients, \\(\\widehat{\\beta}_{p\\times1}\\) has the following solution:\n\\[\n\\widehat\\beta_{p\\times 1} = (X_{p\\times n}^TX_{n \\times p})^{-1}X_{p \\times n}^T \\vec{y}_{n \\times 1}\n\\] Observe the subscripts for the dimensions of the matrices, ending with a \\(p \\times 1\\) vector for the coefficients that minimize the SSE.\nHere, we use carat(Q), depth(Q), table(Q), and color(Q) to predict price.\n\n\n\n\n\n\nNote\n\n\n\nWhen a categorical variable like color is included in a regression model, R converts it into multiple indicator (dummy) variables, one for each level except the reference level (usually the first alphabetically, unless manually changed).\nEach dummy variable shifts the intercept of the regression line for that level, while the slope(s) for the numeric predictors remain the same. This means:\n\nR is effectively fitting a separate line of best fit for each level of the categorical variable — all with the same slope, but different intercepts.\n\nFor example, including color in lm(price ~ carat + depth + table + color) produces:\n\nOne baseline line (intercept) for the reference group (e.g., color = \"D\").\nAdditional parallel lines for each other color group (e.g., E, F, G, etc.), each shifted vertically by its corresponding coefficient (e.g., colorE, colorF, etc.).\n\nSo if the coefficient for colorE is -500, then diamonds with color E are estimated to be $500 less expensive than D-colored diamonds at the same carat, depth, and table values.\nThis approach captures group differences in the starting point (intercepts) of the response, while assuming the effect of continuous predictors (e.g. carat) is constant across all groups.\n\n\n\nlm_multi &lt;- lm(price ~ carat + depth + table + color, data = df)\nsummary(lm_multi)\n\n\nCall:\nlm(formula = price ~ carat + depth + table + color, data = df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-9095.0  -787.8   -80.5   568.3  7873.0 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 10739.61    2692.30   3.989 7.12e-05 ***\ncarat        8251.22      97.16  84.920  &lt; 2e-16 ***\ndepth        -112.34      33.40  -3.364 0.000799 ***\ntable        -115.68      20.77  -5.569 3.30e-08 ***\ncolor.L     -1555.67     146.23 -10.639  &lt; 2e-16 ***\ncolor.Q      -843.07     133.77  -6.303 4.40e-10 ***\ncolor.C       -85.92     129.08  -0.666 0.505778    \ncolor^4        75.10     117.70   0.638 0.523580    \ncolor^5      -123.89     116.66  -1.062 0.288500    \ncolor^6       -72.64     104.92  -0.692 0.488903    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1385 on 990 degrees of freedom\nMultiple R-squared:  0.8842,    Adjusted R-squared:  0.8832 \nF-statistic: 840.1 on 9 and 990 DF,  p-value: &lt; 2.2e-16\n\n\nYou can compare \\(R^2\\) values and p-values to determine whether the additional variables meaningfully improve the model.\n\nAdding Interaction Terms\nWhat if the effect of one variable depends on another? For example, maybe the impact of carat on price differs across color levels. In that case, we can include an interaction term in the model:\n\nlm_interact &lt;- lm(price ~ carat * color, data = df)\nsummary(lm_interact)\n\n\nCall:\nlm(formula = price ~ carat * color, data = df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-9236.8  -767.5   -13.5   648.9  8199.4 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   -2642.59      99.53 -26.551  &lt; 2e-16 ***\ncarat          8119.05     103.78  78.236  &lt; 2e-16 ***\ncolor.L        -132.35     306.56  -0.432 0.666033    \ncolor.Q         315.80     290.51   1.087 0.277286    \ncolor.C         -16.57     275.23  -0.060 0.952018    \ncolor^4         377.23     245.62   1.536 0.124911    \ncolor^5         593.39     239.84   2.474 0.013526 *  \ncolor^6        -225.89     209.92  -1.076 0.282167    \ncarat:color.L -1533.77     313.82  -4.887 1.19e-06 ***\ncarat:color.Q -1137.53     294.94  -3.857 0.000122 ***\ncarat:color.C   110.35     287.00   0.384 0.700695    \ncarat:color^4  -311.72     262.03  -1.190 0.234479    \ncarat:color^5  -865.55     257.40  -3.363 0.000802 ***\ncarat:color^6   255.92     222.64   1.149 0.250641    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1375 on 986 degrees of freedom\nMultiple R-squared:  0.8864,    Adjusted R-squared:  0.8849 \nF-statistic: 591.9 on 13 and 986 DF,  p-value: &lt; 2.2e-16\n\n\n\n\n\n\n\n\nTip\n\n\n\n\nEach level of the color factor has its own intercept and its own slope for carat.\nThe reference group (e.g., color = “D”) uses the coefficient for carat directly:\n\n\\[ \\text{Slope}_D = \\beta_{carat} \\]\n\nFor other groups, the slope is:\n\n\\[ \\text{Slope}_{\\text{color}} = \\beta_{carat} + \\beta_{\\text{carat:color}} \\]\nExample: If carat has a coefficient of 8000 and carat:colorE has a coefficient of -1000, then for color E diamonds:\n\nIntercept = Intercept + \\(\\beta_{\\text{colorE}}\\)\nSlope = $8000 - 1000 = $7000\n\nSo carat still increases price for color E diamonds, but not as sharply as it does for the reference group (D).\n\n\nVisualizing Interactions\n\nYou can visualize the effect of interaction terms by plotting separate regression lines for each group:\n\nggplot(df, aes(x = carat, y = price, color = color)) +\n  geom_point(alpha = 0.5) +\n  geom_smooth(method = \"lm\", formula = y ~ x, se = FALSE) +\n  labs(\n    title = \"Separate Regression Lines by Diamond Color\",\n    x = \"Carat\",\n    y = \"Price (USD)\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nNote: Each color group has its own slope and intercept, as determined by the interaction model.\n\n\n\n\n\n\nNote\n\n\n\nMultiple linear regression allows us to model relationships between one response variable and many predictors.\nCategorical variables create parallel lines (same slope, different intercepts) unless interaction terms are added.\nInteraction terms create separate lines of best fit (different slopes and intercepts) for each group.\n\n\n\n\n\n\nGoodness of Fit\nWe use several metrics to assess the quality of a regression model:\n\n\\(R^2\\): Proportion of variance in the response explained by the predictors.\nResidual Standard Error (RSE): Average size of the residuals.\nF-statistic: Overall significance of the regression.\nResidual Plots: Visual diagnostics to assess assumptions.\n\n\n# Residual plot\nplot(lm_multi, which = 1)  # Residuals vs Fitted\n\n\n\n\n\n\n\n\n\n# Histogram of residuals\nresiduals &lt;- resid(lm_multi)\nhist(residuals, breaks = 30, col = \"lightblue\", main = \"Histogram of Residuals\", xlab = \"Residual\")\n\n\n\n\n\n\n\n\nWe want residuals to be roughly normally distributed and randomly scattered around zero to satisfy assumptions of linear regression.\n\n\nANOVA\nAnalysis of Variance (ANOVA) is a statistical method used to compare the means of three or more groups to determine if at least one of the group means is significantly different from the others.\n\n\nTheoretical Foundation\nANOVA works by partitioning the total variability in the data into two components: - Between-group variability: how much the group means differ from the overall mean. - Within-group variability: how much individual observations vary within each group.\nThe core idea is that if the between-group variability is large relative to the within-group variability, then at least one group mean is likely different.\n[ F = = ]\nWhere: - ( SSB ) = Sum of Squares Between - ( SSW ) = Sum of Squares Within - ( k ) = number of groups - ( n ) = total number of observations\nIf the calculated F-statistic is large, and the p-value is small (typically &lt; 0.05), we reject the null hypothesis:\n\n( H_0: _1 = _2 = = _k ) (all group means are equal)\n( H_A: ) At least one group mean is different\n\n\n\n\nExample: Do Different Diamond Cuts Have Different Average Prices?\nWe’ll use the diamonds dataset and compare the mean price across the five levels of the cut variable.\n\n# Sample for speed\nset.seed(206)\ndf &lt;- diamonds %&gt;% sample_n(1000)\n\n# Summary statistics by cut\ndf %&gt;%\n  group_by(cut) %&gt;%\n  summarise(mean_price = mean(price), n = n())\n\n# A tibble: 5 × 3\n  cut       mean_price     n\n  &lt;ord&gt;          &lt;dbl&gt; &lt;int&gt;\n1 Fair           3677.    28\n2 Good           3429.    74\n3 Very Good      4561.   226\n4 Premium        4355.   273\n5 Ideal          3590.   399\n\n\n\n\n\nRun ANOVA Test\n\n# One-way ANOVA: price ~ cut\nanova_model &lt;- aov(price ~ cut, data = df)\nsummary(anova_model)\n\n             Df    Sum Sq  Mean Sq F value Pr(&gt;F)  \ncut           4 1.996e+08 49901236   3.065  0.016 *\nResiduals   995 1.620e+10 16283224                 \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThis will output the F-statistic and p-value. A small p-value (e.g. &lt; 0.05) suggests that at least one cut has a significantly different mean price.\n\n\n\n\nVisualize the Group Differences\n\nggplot(df, aes(x = cut, y = price)) +\n  geom_boxplot(fill = \"lightblue\") +\n  labs(title = \"Boxplot of Diamond Price by Cut\",\n       x = \"Cut\", y = \"Price ($)\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nBoxplots help visualize both the median and spread of price within each cut level.\n\n\n\nFollow-Up: Which Cuts Are Different?\nIf the ANOVA result is significant, we can follow up with a Tukey HSD test to identify which group pairs differ.\n\nTukeyHSD(anova_model)\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = price ~ cut, data = df)\n\n$cut\n                        diff        lwr        upr     p adj\nGood-Fair         -247.90830 -2694.6097 2198.79307 0.9987125\nVery Good-Fair     884.55594 -1324.7679 3093.87977 0.8096253\nPremium-Fair       678.19322 -1510.0655 2866.45192 0.9157755\nIdeal-Fair         -86.59085 -2242.4691 2069.28738 0.9999672\nVery Good-Good    1132.46424  -344.4875 2609.41597 0.2227928\nPremium-Good       926.10152  -519.1496 2371.35262 0.4030816\nIdeal-Good         161.31745 -1234.4209 1557.05582 0.9978434\nPremium-Very Good -206.36272 -1198.0859  785.36051 0.9795621\nIdeal-Very Good   -971.14679 -1889.2153  -53.07827 0.0320166\nIdeal-Premium     -764.78408 -1630.9331  101.36495 0.1126355\n\n\nThis test controls the family-wise error rate and gives pairwise confidence intervals and p-values.\n\n\n\nInterpretation\n\nIf p &lt; 0.05 in the ANOVA, we conclude that at least one cut differs in mean price.\nUse TukeyHSD to find out which cuts are significantly different.\nANOVA assumes:\n\nIndependent observations\nNormally distributed residuals\nEqual variances across groups (can check with Levene’s test)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Jonathan Day",
    "section": "",
    "text": "I’m CPT Jonathan Day, an educator and data scientist at the United States Military Academy at West Point. I specialize in developing innovative tools and educational resources at the intersection of mathematics, statistics, and computer science.\nMy work spans multiple domains including:\n\nData Science & Machine Learning: Building intelligent systems for literature review automation and research gap identification\nComputer Vision: Developing specialized image analysis tools for materials science\nEducation: Creating comprehensive statistical computing resources for cadets\nScientific Computing: Applying advanced algorithms to solve real-world problems"
  },
  {
    "objectID": "index.html#about-me",
    "href": "index.html#about-me",
    "title": "Jonathan Day",
    "section": "",
    "text": "I’m CPT Jonathan Day, an educator and data scientist at the United States Military Academy at West Point. I specialize in developing innovative tools and educational resources at the intersection of mathematics, statistics, and computer science.\nMy work spans multiple domains including:\n\nData Science & Machine Learning: Building intelligent systems for literature review automation and research gap identification\nComputer Vision: Developing specialized image analysis tools for materials science\nEducation: Creating comprehensive statistical computing resources for cadets\nScientific Computing: Applying advanced algorithms to solve real-world problems"
  },
  {
    "objectID": "index.html#featured-projects",
    "href": "index.html#featured-projects",
    "title": "Jonathan Day",
    "section": "Featured Projects",
    "text": "Featured Projects\n\n\n\n\n🔬 fiberL\nComputer Vision for Fiber Analysis\nAn advanced image processing application that automatically analyzes Scanning Electron Microscope (SEM) images of fiber networks. Uses sophisticated computer vision algorithms to detect, measure, and characterize fiber structures.\nKey Features:\n\nAdvanced morphological image processing\nIntelligent fiber network reconstruction\nQuantitative metrics (length, orientation, persistence length)\nInteractive Streamlit interface\nStatistical analysis and visualization\n\nTechnologies: Python, OpenCV, scikit-image, Streamlit\nView Project → GitHub\n\n\n\n\n\n\n✈️ WUC Predictor\nKC-135 Maintenance Code Classification\nAn AI-powered application that predicts Work User Codes (WUCs) for KC-135 aircraft maintenance discrepancies using NLP. Automatically classifies maintenance issues into proper system codes based on TM 1C-135-06 technical manual.\nKey Features:\n\nFine-tuned transformer model for WUC classification\nSingle discrepancy and batch CSV processing\nConfidence scores with detailed system information\n34 aircraft system categories (airframe, electrical, hydraulic, etc.)\nDeployed on Hugging Face (jonday/wuc-model)\n\nTechnologies: Python, Streamlit, PyTorch, Transformers\nGitHub\n\n\n\n\n\n\n📈 HedgeAbove\nAI-Powered Finance Analytics Platform\nA comprehensive investment platform combining machine learning forecasting with quantitative finance techniques. Provides institutional-grade analytics for portfolio optimization, risk management, and AI-driven predictions.\nKey Features:\n\nStock screener with 2,535+ stocks and 50+ metrics\nReal-time portfolio tracking with P&L analytics\nAdvanced risk analytics (VaR, CVaR, Sharpe ratio)\nAI price predictions (LSTM, Prophet, ARIMA)\nPortfolio optimization (Efficient Frontier, Max Sharpe)\nOptions pricing with full Greeks analysis\n\nTechnologies: Python, Streamlit, TensorFlow, Plotly, yFinance\nGitHub\n\n\n\n\n\n\n📚 MedScrape\nAI-Powered Literature Review Platform\nAn intelligent system that automates PubMed literature searches, performs unsupervised clustering of research articles, and identifies research gaps using natural language processing and machine learning.\nKey Features:\n\nAutomated PubMed database scraping\nMultiple clustering algorithms (K-Means, DBSCAN, LDA)\nResearch gap identification\nInteractive 3D visualizations\nExtractive summarization\n\nTechnologies: Python, Streamlit, scikit-learn, Gensim, UMAP\nView Project → GitHub Live Demo\n\n\n\n\n\n\n📊 MA206: Probability & Statistics\nComprehensive Course Guide\nA complete educational resource for Introduction to Probability and Statistics, featuring interactive R code examples, comprehensive statistical coverage from exploratory data analysis to ANOVA, and the Six Step Method framework.\nTopics Covered:\n\nData and Randomness\nUnivariate Inference\nMultivariate Inference\nR Programming with tidyverse\nStatistical Visualization\n\nTechnologies: Quarto, R, RStudio, tidyverse\nView Course → Code Annex →\n\n\n\n\n\n\n♟️ Chess Opening Trainer\nMemorization Through Active Practice\nAn interactive Streamlit application designed to help chess players master their opening repertoire through quiz modes, spaced repetition, and random position testing.\nKey Features:\n\nThree training modes (Study, Quiz, Random Test)\nSpaced repetition system\nProgress tracking with mastery indicators\nInteractive chess board visualization\nAuto-play with adjustable speed\n\nTechnologies: Python, Streamlit, python-chess\nGitHub"
  },
  {
    "objectID": "index.html#technical-skills",
    "href": "index.html#technical-skills",
    "title": "Jonathan Day",
    "section": "Technical Skills",
    "text": "Technical Skills\n\n\nProgramming Languages\n\nPython (Advanced)\nR (Advanced)\nSQL\nJavaScript\n\n\n\nData Science & ML\n\nscikit-learn\nTensorFlow/PyTorch\nNLP (Gensim, NLTK)\nComputer Vision (OpenCV)\nStatistical Analysis\n\n\n\nTools & Frameworks\n\nStreamlit\nQuarto/RMarkdown\nGit/GitHub\nDocker\nCloud Deployment"
  },
  {
    "objectID": "index.html#contact",
    "href": "index.html#contact",
    "title": "Jonathan Day",
    "section": "Contact",
    "text": "Contact\n\nEmail: jonathan.day@westpoint.edu\nGitHub: github.com/lonespear\nInstitution: United States Military Academy, West Point, NY\n\n\nLast updated: December 2024"
  },
  {
    "objectID": "course_guide_backup.html",
    "href": "course_guide_backup.html",
    "title": "MA206: Introduction to Probability and Statistics",
    "section": "",
    "text": "To Cadets:\nThis course, MA206, introduces you to the foundational principles of probability and statistics, emphasizing data literacy and inference. It begins with Block I, covering data types, visualization, and basic probability rules including counting and the behavior of random variables. Block II builds on this by exploring discrete and continuous distributions, the Central Limit Theorem, and tools for one-sample inference such as confidence intervals and hypothesis testing for proportions and means. Finally, Block III develops cadets’ ability to analyze relationships between variables through two-sample tests, linear regression, ANOVA, and goodness-of-fit testing. By the end of the course, cadets will be equipped to make sound, data-driven decisions grounded in statistical reasoning.\nEvery statistical analysis begins with a clear, repeatable process that transforms curious questions into reliable insights. The Six Step Method serves as your roadmap—guiding you from formulating a meaningful research question through data collection, exploration, inference, and finally thoughtful reflection—so that every conclusion you draw is both rigorous and reproducible.\nSix Step Method\nA Note on Technology:\nIn this course the primary tool used for data analysis is R. Throughout this course you will implement techniques for summarizing, visualizing, and analyzing data. The primary focus of this course is not for you to become masters in coding, however building on skills learned in CY105 will help your analysis in understanding how to use information technology to demonstrate successful outcomes in this course."
  },
  {
    "objectID": "course_guide_backup.html#types-of-data-sampling-and-bias",
    "href": "course_guide_backup.html#types-of-data-sampling-and-bias",
    "title": "MA206: Introduction to Probability and Statistics",
    "section": "Types of Data, Sampling, and Bias",
    "text": "Types of Data, Sampling, and Bias\nFundamental to statistical analysis is understanding the types of data we encounter, the methods we use to collect them, and the potential sources of bias that can undermine the validity of our conclusions. We distinguish between categorical (qualitative) and quantitative (numerical) data. Categorical variables can further be broken down into nominal (color, baseball position, type of animal), ordinal (I had a very bad/somewhat bad/neutral/good/very good experience at CFT), and binary (Yes I passed Air Assault/No I did not). Quantitative data can also be broken down into either discrete or continuous. Understanding these distinctions is critical for selecting the correct tools for analysis and interpretation.\nThe two methods of sampling in this course will be through simple random sampling or convenience sampling. The difference in application is whether we can generalize our results to the larger population. As an example, say I do not have an exhaustive list of cadet ID numbers to randomly select from. Instead, I only can stand in Central Area after class at 1630 and survey the first 50 cadets that willingly take my survey. There is a certain sub-population I am probably missing (1st Reg, 4th Reg, Corps Squad, etc). This is a convenience sample. Instead, if I randomly select 50 C-Numbers from a complete list of the Corps obtained from the registrar, this would be a a true random sample of the Corps, whereas the former is what is known as selection bias.\nFinally, we explore the concept of bias in data collection. We identify common sources such as selection bias, response bias, and measurement bias, and discuss how poor sampling practices or flawed survey design can distort findings. This lesson sets the stage for the rest of the course by highlighting the importance of thoughtful data collection and critical evaluation of data sources.\nAs an example let us look at a dataset aggregated from over 50,000 diamonds:\n\n\n\n\n\ncarat\ncut\ncolor\nclarity\ndepth\ntable\nprice\nx\ny\nz\n\n\n\n\n1.02\nGood\nG\nVS2\n63.8\n59.0\n6080\n6.34\n6.27\n4.02\n\n\n0.31\nIdeal\nF\nVVS1\n61.9\n53.5\n882\n4.36\n4.39\n2.71\n\n\n0.60\nPremium\nD\nSI2\n61.3\n61.0\n1428\n5.46\n5.40\n3.33\n\n\n0.41\nIdeal\nE\nIF\n62.1\n54.0\n1419\n4.75\n4.81\n2.97\n\n\n0.72\nVery Good\nH\nVS1\n62.2\n54.0\n2877\n5.74\n5.76\n3.57\n\n\n1.20\nIdeal\nF\nVS2\n62.6\n56.0\n8486\n6.78\n6.73\n4.23\n\n\n\n\n\nEach of the rows is an individual diamond, generally called an observation. Each of the columns are unique aspects measured for every observation, called variables. Variables are either categorical, qualitative aspects of each measurement, or quantitative, a numbered entry."
  },
  {
    "objectID": "course_guide_backup.html#exploratory-data-analysis",
    "href": "course_guide_backup.html#exploratory-data-analysis",
    "title": "MA206: Introduction to Probability and Statistics",
    "section": "Exploratory Data Analysis",
    "text": "Exploratory Data Analysis\nUnderstanding, communicating, and interpreting your data is paramount to any initial data analysis project. These are done through numerous visualizations and summary statistics which we will learn to regularly implement when given any new dataset.\n\nOne Variable – Visualizations and Summary Statistics\nStarting with a variable-by-variable approach is a natural first step. This is done rapidly in R with the following few functions:\n\nHistograms\n\n\n\n\n\n\n\n\n\nHistograms tell us where most of the values for a quantitative variable lie in its given distribution. We can determine skewness, a measure of how lopsided the data appear or if there are any asymmetries or tails.\n\n\nBoxplots\n\n\n\n\n\n\n\n\n\nSimilar to a histogram, a boxplot will tell us exactly where the median, 1st and 3rd quartiles, and outliers exist for any quantitative variable. The ‘whiskers’ are determined by \\(1.5 \\times IQR\\) where the inter-quartile range is the \\(3rd - 1st\\) quartiles.\n\n\nSummary Statistics\n\n\n\n\n\nVariable\nMean\nMedian\nSD\nVar\nMin\nMax\n\n\n\n\ncarat\n0.79\n0.70\n0.46\n0.21\n0.23\n3.24\n\n\ndepth\n61.77\n61.90\n1.40\n1.97\n56.30\n68.90\n\n\ntable\n57.48\n57.00\n2.26\n5.11\n50.00\n66.00\n\n\nprice\n3873.55\n2387.00\n3912.55\n15308046.06\n337.00\n18692.00\n\n\nx\n5.72\n5.68\n1.10\n1.21\n3.88\n9.44\n\n\ny\n5.72\n5.68\n1.09\n1.20\n3.90\n9.40\n\n\nz\n3.53\n3.52\n0.68\n0.46\n2.39\n5.85\n\n\n\n\n\nThe above are the predominant statistics you want to discern for every quantitative variable in your dataset. The benchmark location statistics are the mean, median, max, and min, while the standard deviation and variance are measures of how spread out the data are relative to one another.\n\\[\n\\begin{align}\n\\text{Sample Mean: } \\ \\ & \\bar{X} = \\frac{1}{n}\\sum_{i=1}^n x_i \\\\\n\\text{Sample Variance: } \\ \\ &  S^2 = \\frac{1}{n-1}\\sum_{i=1}^n ( x_i - \\bar{X}  )^2 \\\\\n\\text{Sample Standard Deviation:} \\ \\ & s = \\sqrt{\\frac{1}{n-1}\\sum_{i=1}^n ( x_i - \\bar{X} )^2}\n\\end{align}\n\\]\nNote: the standard deviation is the square root of the variance\n\n\n\nTwo Variables – Visualizations and Summary Statistics\n\nScatter Plots\n\n\n\n\n\n\n\n\n\nA scatterplot is the main tool to visualize and identify a relationship between two quantitative variables. Oftentimes, coloring each observation by another categorical variable is a way to maximize effectiveness of a single plot, as you are encoding more information within the same space.\n\n\nBar Plots\n\n\n\n\n\n\n\n\n\nBar charts are visualizations when focusing your audience on a single categorical and a summary statistic or aspect of a quantitative variable. In practice, scatter-plots show more information since you can encode the categorical variable through another channel such as color, size, shape while keeping the horizontal and vertical axis for two separate quantitative variables.\n\n\nCorrelation\n\n\n\n\n\n\ncarat\ndepth\ntable\nprice\n\n\n\n\ncarat\n1.00\n0.00\n0.22\n0.91\n\n\ndepth\n0.00\n1.00\n-0.32\n-0.02\n\n\ntable\n0.22\n-0.32\n1.00\n0.17\n\n\nprice\n0.91\n-0.02\n0.17\n1.00\n\n\n\n\n\nCorrelation is the only multivariate summary statistic we will be using in this course, used to describe how two variables tend to move in tandem with one another. A perfect linear association evokes a correlation of 1, the opposite being a perfect negative association with a correlation of -1. No association is implied by a correlation near 0.\nMathematically: &gt; Definition &gt; For any two variables X,Y, the correlation of X and Y are: \\[\nr = \\frac{\\sum_{i=1}^n (x_i - \\bar{x})(y_i-\\bar{y})}{\\sum_{i=1}^n (x_i - \\bar{x})^2\\sum_{i=1}^n (y_i - \\bar{y})^2}\n\\]"
  },
  {
    "objectID": "course_guide_backup.html#probability",
    "href": "course_guide_backup.html#probability",
    "title": "MA206: Introduction to Probability and Statistics",
    "section": "Probability",
    "text": "Probability\n\nSample Space and Random Experiment (\\(\\Omega\\))\nA random experiment is a process that produces an outcome which cannot be predicted with certainty in advance. It must be well-defined, have more than one possible outcome, and be repeatable under similar conditions. Each performance of the experiment results in a single outcome from the sample space. The sample space is the set of all possible outcomes of a random experiment.\n\n\nDefinition:\n\nThe sample space is the set of all possible outcomes of a random experiment, denoted \\(\\Omega\\).\nAn event is a subset of the sample space. It can represent one or more outcomes.\nIf all outcomes in \\(\\Omega\\) are equally likely, then for any event \\(A\\):\n\n\n\\[\n\\mathbb{P}(A) = \\frac{\\text{Number of outcomes in } A}{\\text{Total outcomes in } \\Omega}\n\\]\n\nExamples:\n\nTossing a coin once: \\(\\Omega = {\\text{Heads}, \\text{Tails}}\\)\nRolling a 6-sided die: \\(\\Omega = {1, 2, 3, 4, 5, 6}\\)\nLetter grade in MA206: \\(\\Omega = {A, B, C, D, F}\\)\nNumber of emails received in an hour: \\(\\Omega = {0, 1, 2, \\dots}\\)\n\n\n\n\nProbability Measure (\\(\\mathbb{P}\\))\nA probability measure is a rule, denoted \\(\\mathbb{P}\\), that assigns a number between 0 and 1 to every event in a collection of events (called a sigma-algebra, denoted \\(\\mathcal{F}\\) ). These probabilities must follow three key rules, known as the axioms of probability.\n\n\n\n\n\n\n\nNoteAxioms of Probability\n\n\n\n\nNon-Negativity\nFor any event \\(A\\), the probability is never negative:\n\\[\n\\mathbb{P}(A) \\geq 0\n\\]\nNormalization\nThe probability of one of the events happening over the entire sample space is 1:\n\\[\n\\mathbb{P}(\\Omega) = 1\n\\]\nAdditivity (for disjoint events)\nIf events \\(A_1, A_2, A_3, \\dots\\) are mutually exclusive (no overlap), then the probability that any one of them occurs is the sum of their individual probabilities:\n\\[\n\\mathbb{P}\\left(\\bigcup_{i=1}^{\\infty} A_i\\right) = \\mathbb{P}(A_1) + \\mathbb{P}(A_2) + \\mathbb{P}(A_3) + \\cdots\n\\]\n\n\n\n\nExample (Simple):\nLet \\(A\\), \\(B\\), and \\(C\\) be outcomes when rolling a die:\n\n\\(A = \\{1\\}\\), $B = {3} $, \\(C = \\{5\\}\\)\n\nThese are disjoint events (they don’t overlap).\n\nThen: \\[\n\\mathbb{P}(A \\cup B \\cup C) = \\mathbb{P}(A) + \\mathbb{P}(B) + \\mathbb{P}(C) = \\frac{1}{6} + \\frac{1}{6} + \\frac{1}{6} = \\frac{1}{2}\n\\]\n\nThese three rules form the mathematical foundation of all probability calculations — everything else builds on them.\n\n\n\n\n\n\n\nNoteSet Theory\n\n\n\n\nThe complement of an event \\(A\\), written \\(A^c\\), consists of all outcomes in \\(\\Omega\\) that are not in \\(A\\):\n\n\\[\n\\mathbb{P}(A) + \\mathbb{P}(A^c) = 1\n\\]\n\nThe intersection \\(A \\cap B\\) consists of outcomes where both \\(A\\) and \\(B\\) occur.\nThe union \\(A \\cup B\\) consists of outcomes where either \\(A\\), \\(B\\), or both occur:\n\n\\[\n\\mathbb{P}(A \\cup B) = \\mathbb{P}(A) + \\mathbb{P}(B) - \\mathbb{P}(A \\cap B)\n\\]\n\nTwo events \\(A\\) and \\(B\\) are disjoint (mutually exclusive) if they cannot both occur:\n\n\\[\nA \\cap B = \\varnothing \\quad \\text{and} \\quad \\mathbb{P}(A \\cap B) = 0\n\\]\n\n\n\n\nConditional Probability\nFor events \\(A\\) and \\(B\\) with \\(0 &lt; P(B) \\le 1\\), the conditional probability of \\(A\\) given \\(B\\) is:\n\\[\nP(A \\mid B) = \\frac{P(A \\cap B)}{P(B)}\n\\]\nExample: One card is drawn from a standard deck.\nLet \\(A\\): card is a Queen, and \\(B\\): card is a face card.\nFind \\(P(A)\\), \\(P(B)\\), and \\(P(A \\mid B)\\).\n\n\n\nLaw of Total Probability\n\n\n\n\n\n\nNoteLoTP\n\n\n\nIf \\(E_1, \\dots, E_n\\) is a partition of the sample space (mutually exclusive and exhaustive), then for any event \\(A\\):\n\\[\nP(A) = \\sum_{i=1}^{n} P(E_i) P(A \\mid E_i)\n\\]\n\n\nExample:\nA fair die is rolled. Let event A: “an even number is rolled”.\nLet:\n- \\(E_1\\): roll is 1 or 2\n- \\(E_2\\): roll is 3 or 4\n- \\(E_3\\): roll is 5 or 6\n\nFind:\n- \\(P(A \\mid E_1)\\), \\(P(A \\mid E_2)\\), \\(P(A \\mid E_3)\\)\n- Then use the Law of Total Probability to find \\(P(A)\\)\n\n\n\nBayes’ Theorem\n\n\n\n\n\n\nNoteSwitching the Conditioning of Events\n\n\n\n\nDefinition:\nIf \\(E_1, \\dots, E_n\\) is a partition of the sample space and \\(P(A) &gt; 0\\), then:\n\n\\[\nP(E_k \\mid A) = \\frac{P(A \\mid E_k) P(E_k)}{\\sum_{i=1}^n P(A \\mid E_i) P(E_i)}\n\\]\n\n\nExample:\nTwo urns:\n- Urn 1: 1 red, 1 blue\n- Urn 2: 3 red, 1 blue\nPick an urn at random, then draw one ball.\nIf the ball is red, what is the probability it came from Urn 1?\n\n\n\nCounting Principles\nBefore we can begin a thorough treatment of probability, some concepts in counting are needed to identify four common situations. These arise depending on when things are “allowed” to repeat or the “order” items are chosen in matters. The ability to discern when these four situations arise is more than half the battle.\n\nOrdered with Replacement\nThink of the number of ways of choosing a 4-digit passcode on your phone.\nThe order of the numbers matters, and you are allowed to repeat the same number. So how many arrangements are there? Since repetition is allowed and order matters, there are 10 digits for each position, giving:\n\\[\n\\text{Ordered Arrangements with Replacement} = n^r = 10^4 = 10{,}000\n\\]\n\n\nOrdered without Replacement\nThink of the number of ways I can create a batting order from 9 position players.\nThe order still matters, but players cannot be repeated. This is a permutation — an ordered arrangement without replacement.\n\\[\n{}_nP_r = P(n, r) = \\frac{n!}{(n - r)!}\n\\]\nNote: Factorial in math is computed as \\(5! = 5 \\times 4 \\times 3 \\times 2 \\times 1 = 120\\).\nFor example, the number of ways to assign the first 3 batting positions from 9 players:\n\\[\n{}_9P_3 = \\frac{9!}{(9-3)!} = 9 \\times 8 \\times 7 = 504\n\\]\n\n\nUnordered without Replacement\nThink of how many ways you can choose 3 scoops of ice cream from 5 unique flavors without repeats.\nBecause order doesn’t matter and repeats aren’t allowed, we use combinations:\n\\[\n{}_nC_k = \\binom{n}{k} = \\frac{n!}{k!(n-k)!}\n\\]\n\n\nUnordered with Replacement\nThink of how many different combinations of 3 scoop ice cream cones you can make with 5 unique flavors while allowing repeats.\nThis is the trickiest scenario. The order doesn’t matter, and repetition is allowed. The formula is:\n\\[\n\\text{Unordered Arrangements with Replacement} = \\binom{r+n-1}{r} = \\frac{(r+n-1)!}{r!(n-1)!}\n\\]\nExample: choosing 3 scoops from 5 flavors (with repeats):\n\\[\n\\binom{3+5-1}{3} = \\binom{7}{3} = 35\n\\]\nThis can be understood using the stars and bars method: selecting \\(r\\) scoops with \\(n-1\\) dividers. Imagine representing each scoop as a ★ (star) and using vertical bars | to separate flavor types. To choose \\(r\\) scoops from \\(n\\) flavors, you need \\(r\\) stars (for the scoops) and \\(n - 1\\) bars (to divide them into \\(n\\) categories). For example, if \\(r = 3\\) scoops and \\(n = 5\\) flavors, you arrange 3 stars and 4 bars in a row. One possible arrangement is ★ | ★★ | | |, which represents 1 scoop of flavor 1, 2 scoops of flavor 2, and 0 scoops of flavors 3, 4, and 5. The number of such arrangements is given by the combination formula \\(\\binom{r + n - 1}{r}\\), since you are choosing positions for the \\(r\\) indistinguishable stars among the \\(r + n - 1\\) total positions (stars and bars combined).\nNote: The above section may seem like it came out of nowhere, that is okay. A fundamental difficulty in probability is finding the sample space or event space due to finding the various different combinations/permutations sequences of different possibilities. To elaborate consider the next example: \n\nExample: No Matching Pairs in a Shoe Sample\nA closet contains \\(n\\) pairs of shoes (so \\(2n\\) total shoes). If \\(2r\\) shoes are chosen at random (where \\(2r &lt; n\\)), what is the probability that no matching pair is selected?\nWe are selecting \\(2r\\) shoes such that no left and right shoe from the same pair are both chosen.\nStrategy:\n1. First choose \\(2r\\) distinct pairs from the \\(n\\) available — there are \\(\\binom{n}{2r}\\) ways to do this.\n2. From each of these \\(2r\\) selected pairs, choose only one shoe (either left or right) — there are \\(2^{2r}\\) ways to do this.\n3. The total number of ways to choose any \\(2r\\) shoes out of \\(2n\\) is \\(\\binom{2n}{2r}\\).\nSo, the desired probability is:\n\\[\n\\mathbb{P}(\\text{No matching pair}) = \\frac{\\binom{n}{2r} \\cdot 2^{2r}}{\\binom{2n}{2r}}\n\\]"
  },
  {
    "objectID": "course_guide_backup.html#random-variables-expectation-and-variance",
    "href": "course_guide_backup.html#random-variables-expectation-and-variance",
    "title": "MA206: Introduction to Probability and Statistics",
    "section": "Random Variables, Expectation, and Variance",
    "text": "Random Variables, Expectation, and Variance\n\n\n\n\n\n\nNoteRandom Variable\n\n\n\nA random variable is a mapping that assigns a real number to every outcome in the sample space:\n\\[\nX: \\Omega \\rightarrow \\mathbb{R}\n\\]\n\n\n\n\n\n\n\n\nNoteCumulative Distribution Function\n\n\n\nA cumulative distribution function (CDF) is a function \\(F_X: \\mathbb{R} \\rightarrow [0,1]\\) defined by:\n\\[\nF_X(x) = \\mathbb{P}(X \\le x)\n\\]"
  },
  {
    "objectID": "course_guide_backup.html#discrete-random-variables",
    "href": "course_guide_backup.html#discrete-random-variables",
    "title": "MA206: Introduction to Probability and Statistics",
    "section": "Discrete Random Variables",
    "text": "Discrete Random Variables\n\n\n\n\n\n\nNoteDiscrete Random Variables\n\n\n\nA discrete random variable is a random variable that takes countably many values in \\(\\mathbb{R}\\).\nIts probability mass function is given by:\n\\[\nf_X(x) = \\mathbb{P}(X = x)\n\\]\nThe expected value (mean) of a discrete random variable \\(X\\) is:\n\\[\n\\mathbb{E}[X] = \\sum_x x \\cdot \\mathbb{P}(X = x)\n\\]\nDefinition (Variance):\nThe variance of a discrete random variable \\(X\\) is:\n\\[\n\\mathrm{Var}(X) = \\mathbb{E}\\left[(X - \\mathbb{E}[X])^2\\right] = \\sum_x (x - \\mathbb{E}[X])^2 \\cdot \\mathbb{P}(X = x)\n\\]\n\n\n\nBinomial Distribution:\n\nLet \\(X \\sim \\text{Binomial}(n, p)\\) where \\(n \\in \\mathbb{N}\\) and \\(0 &lt; p &lt; 1\\).\n\nProbability Mass Function (PMF):\n\\[\n\\mathbb{P}(X = k) = \\binom{n}{k} p^k (1 - p)^{n - k}, \\quad \\text{for } k = 0, 1, \\dots, n\n\\]\nCumulative Distribution Function (CDF):\n\\[\nF_X(x) = \\mathbb{P}(X \\le x) = \\sum_{k=0}^{\\lfloor x \\rfloor} \\binom{n}{k} p^k (1 - p)^{n - k}\n\\]\nExpected Value:\n\\[\n\\mathbb{E}[X] = np\n\\]\nVariance:\n\\[\n\\mathrm{Var}(X) = np(1 - p)\n\\]\n\n\n\n\nGeometric Distribution:\n\nLet \\(X \\sim \\text{Geometric}(p)\\) be the number of trials until the first success (including the success), where \\(0 &lt; p &lt; 1\\).\n\nProbability Mass Function (PMF):\n\\[\n\\mathbb{P}(X = k) = (1 - p)^{k - 1} p, \\quad \\text{for } k = 1, 2, 3, \\dots\n\\]\nCumulative Distribution Function (CDF):\n\\[\nF_X(x) = \\mathbb{P}(X \\le x) = 1 - (1 - p)^{\\lfloor x \\rfloor}\n\\]\nExpected Value:\n\\[\n\\mathbb{E}[X] = \\frac{1}{p}\n\\]\nVariance:\n\\[\n\\mathrm{Var}(X) = \\frac{1 - p}{p^2}\n\\]"
  },
  {
    "objectID": "course_guide_backup.html#continuous-random-variables",
    "href": "course_guide_backup.html#continuous-random-variables",
    "title": "MA206: Introduction to Probability and Statistics",
    "section": "Continuous Random Variables",
    "text": "Continuous Random Variables\n\n\n\n\n\n\nNoteContinuous Random Variable\n\n\n\nA continuous random variable takes infinitely many values in \\(\\mathbb{R}\\). Its probability density function is given by:\n\\[\nf_X(x) = \\mathbb{P}(X = x)\n\\]\nNote: We do not find probabilities with the pdf like the pmf of a discrete random variable. We integrate over a neighborhood of the support of X, as there is no probability mass at any single point for a continuous distribution.\nThe expected value (mean) of a continuous random variable \\(X\\) is:\n\\[\n\\mathbb{E}[X] = \\int_{-\\infty}^\\infty x f_X(x)dx\n\\]\nDefinition (Variance):\nThe variance of a discrete random variable \\(X\\) is:\n\\[\n\\mathrm{Var}(X) = \\mathbb{E}\\left[(X - \\mathbb{E}[X])^2\\right] = \\int_{-\\infty}^\\infty (x - \\mathbb{E}[X])^2 f_X(x)dx = \\mathbb{E}[X^2] - \\mathbb{E}[X]^2\n\\]\n\n\n\nCentral Limit Theorem (CLT)\nThe Central Limit Theorem (CLT) is one of the most important results in statistics. It states that the sampling distribution of the sample mean \\(\\bar{X}\\) becomes approximately normal as the sample size \\(n\\) increases, regardless of the shape of the population distribution (provided it has finite mean and variance).\nSpecifically, if \\(X_1, X_2, \\dots, X_n\\) are i.i.d. random variables with mean \\(\\mu\\) and variance \\(\\sigma^2\\), then:\n\\[\n\\frac{\\bar{X} - \\mu}{\\sigma / \\sqrt{n}} \\xrightarrow{d} \\mathcal{N}(0,1) \\quad \\text{as } n \\to \\infty\n\\]\nThis justifies the widespread use of the normal distribution to approximate sample means in practice.\n\n\n\n\nCredit: The New York Times\n\n\n\nNormal Distribution\nThe above video showed you the importance of this distribution, also called a Gaussian Distribution. Many natural phenomena are normally distributed.\n\nNormal Distribution\nLet \\(X \\sim \\mathcal{N}(\\mu, \\sigma^2)\\), where \\(\\mu \\in \\mathbb{R}\\) and \\(\\sigma &gt; 0\\).\n\nProbability Density Function (PDF):\n\\[\nf(x) = \\frac{1}{\\sqrt{2\\pi \\sigma^2}} \\exp\\left( -\\frac{(x - \\mu)^2}{2\\sigma^2} \\right), \\quad x \\in \\mathbb{R}\n\\]\nCumulative Distribution Function (CDF):\nThere is no closed-form expression, but it is denoted as:\n\\[\nF_X(x) = \\mathbb{P}(X \\le x) = \\Phi\\left( \\frac{x - \\mu}{\\sigma} \\right)\n\\]\nwhere \\(\\Phi\\) is the standard normal CDF.\nExpected Value:\n\\[\n\\mathbb{E}[X] = \\mu\n\\]\nVariance:\n\\[\n\\mathrm{Var}(X) = \\sigma^2\n\\]\n\n\n\n\nExponential Distribution\nThis distribution is helpful to model continuous time-related events: time between system failures at a factory, time between phone calls at a call center, time between insurance claims received at a insurance firm. All these can be modeled with this useful continuous random variable.\n\nLet \\(X \\sim \\text{Exponential}(\\lambda)\\) with \\(\\lambda &gt; 0\\).\n\nProbability Density Function (PDF):\n\\[\nf(x) = \\lambda e^{-\\lambda x}, \\quad x \\ge 0\n\\]\nCumulative Distribution Function (CDF):\n\\[\nF_X(x) = \\mathbb{P}(X \\le x) = 1 - e^{-\\lambda x}, \\quad x \\ge 0\n\\]\nExpected Value:\n\\[\n\\mathbb{E}[X] = \\frac{1}{\\lambda}\n\\]\nVariance:\n\\[\n\\mathrm{Var}(X) = \\frac{1}{\\lambda^2}\n\\]"
  },
  {
    "objectID": "course_guide_backup.html#confidence-intervals",
    "href": "course_guide_backup.html#confidence-intervals",
    "title": "MA206: Introduction to Probability and Statistics",
    "section": "Confidence Intervals",
    "text": "Confidence Intervals\nA confidence interval (CI) gives a range of plausible values for a population parameter based on a sample statistic. The general structure of any confidence interval is:\n\\[\n\\text{point estimate} \\ \\pm \\ \\text{margin of error}\n\\]\nMore specifically, for large samples or when the sampling distribution of the estimate is approximately normal:\n\\[\n\\text{CI} = \\hat{\\theta} \\ \\pm \\ z^* \\cdot \\text{SE}(\\hat{\\theta})\n\\]\nWhere:\n\n\\(\\hat{\\theta}\\) is the point estimate (e.g., \\(\\bar{x}\\) for the mean, \\(\\hat{p}\\) for a proportion)\n\\(z^*\\) is the critical value from the standard normal distribution (e.g., 1.96 for 95% confidence)\n\\(\\text{SE}(\\hat{\\theta})\\) is the standard error of the estimate\n\nThis structure applies to many common settings:\n\nCI for a population mean: \\[\n\\bar{x} \\pm z^* \\cdot \\frac{s}{\\sqrt{n}}\n\\]\nCI for a population proportion: \\[\n\\hat{p} \\pm z^* \\cdot \\sqrt{\\frac{\\hat{p}(1 - \\hat{p})}{n}}\n\\]\n\n\nInterpretation:\n\n“We are 95% confident that the true population parameter lies within this interval.”\n\nThis does not mean there’s a 95% probability the parameter is in the interval — rather, it means that 95% of all intervals computed from repeated samples in this manner would contain the true parameter."
  },
  {
    "objectID": "course_guide_backup.html#one-sample-hypothesis-testing",
    "href": "course_guide_backup.html#one-sample-hypothesis-testing",
    "title": "MA206: Introduction to Probability and Statistics",
    "section": "One Sample Hypothesis Testing",
    "text": "One Sample Hypothesis Testing\nHypothesis testing is a formal method for making inferences about a population using sample data. The whole test aspect is questioning if the statistic your sample shows is significantly different than a certain value in question. You have two underlying premises, referred to as the null and alternative hypotheses. The null hypothesis assumes that there is no difference: the statistic from your value is the same as the value you are testing. The alternative conflicts the null and says they are different. The process involves:\n\nState the null and alternative hypotheses. There are three different variants to create your hypotheses statements depending on what the question being asked entails.\n\n\nGreater than Alternative Hypothesis\n\n\\[\n\\begin{align}\nH_0: Parameter &= Value \\ in \\ Question \\\\\nH_A: Parameter &&gt; Value \\ in \\ Question\n\\end{align}\n\\]\nThe entire inference aspect of hypothesis testing is that you are using your sample statistic, a tangible aspect of your data, to make an argument about the population parameter, an entity that is unknown to you. This is why the hypotheses are written in terms of the parameter. You are testing whether an aspect or parameter about the population is greater than a benchmark value decided by you in advance.\n\nLess than Alternative Hypothesis\n\n\\[\n\\begin{align}\nH_0: Parameter &= Value \\ in \\ Question \\\\\nH_A: Parameter &&lt; Value \\ in \\ Question\n\\end{align}\n\\]\nVery similarly, the less than hypothesis is also a one-sided hypothesis test in that you are only testing one side of the value, abeit this time if the population parameter is less than the tested value.\n\nNot equal to Alternative Hypothesis\n\n\\[\n\\begin{align}\nH_0: Parameter &= Value \\ in \\ Question \\\\\nH_A: Parameter &\\neq Value \\ in \\ Question\n\\end{align}\n\\]\nThis is the only two-sided hypothesis test, denoted with the not equal to alternative hypothesis. Both sides must be accounted for in this test, and therefore as we will see shortly require more evidence for significance.\n\nChoose a significance level \\(\\alpha\\).\n\nThis is the threshold you will also decide to inform your certainty in your conclusions. As seen previously, unless you are finding the probability that something will happen in the entire sample space (which happens probability 1); ie, there are no absolutes. Therefore there is always a chance that your conclusion will be wrong. Here is where you decide how “often” you are willing to be wrong. Is it 1% of the time? 5% of the time? Think of the significance level as choosing the percentage of the time you are willing to be wrong in your conclusion, (I know this sounds weird). A common \\(\\alpha\\) is 5%.\n\nCompute the test statistic.\n\nCompute the relevant summary statistic. In this course you will either be calculating a sample proportion, denoted \\(\\hat{p}\\) if the variable of interest is categorical or the sample mean \\(\\bar{X}\\) if quantitative.\n\nStandardize the test statistic.\n\nThis step transforms your statistic so it can be treated as a random variable from a named distribution. For proportions this will be a Normal Random Variable, however if quantitative it will be a t-distributed random variable.\n\n\n\n\nCredit: 365 Data Science\n\nDetermine the p-value.\n\nOnce we have the standardized statistic, it can be treated as either a Standard Normal Random Variable or Student’s-t Random Variable. This is where the alternative hypotheses come in to play to determine the p-value: the probability that if the null hypothesis is indeed true you choose to make an argument supporting the alternative. To find the probability of a certain event happening in a continuous random variable, you integrate the probability density function with the limits of integration being the range of values the random variable could take. Both the Standard Normal and Student’s-t are continuous, so depending on your alternative hypothesis, your p-value is calculated by either of the following:\n\\[\n\\begin{align}\n\\text{Greater Hypothesis} &&&& \\text{Less than Hypothesis} &&&& \\text{Two Sided Hypothesis} \\\\\n\\mathbb{P}(X&gt;z) = \\int_z^\\infty f_X(x) dx &&&& \\mathbb{P}(X&lt;z) = \\int_{-\\infty}^z f_X(x)dx &&&& \\mathbb{P}(X&gt;z) = \\int_{|z|}^\\infty f_X(x)dx + \\int_{-\\infty}^{-|z|} f_X(x)dx\n\\end{align}\n\\]\n\n\nIgnoring unknown labels:\n• parse : \"TRUE\"\n\n\n\n\n\n\n\n\n\n\nMaking a conclusion based on comparison.\n\nOnce a p-value is obtained, reference it to the significance level chosen. If the p-value is greater than \\(\\alpha\\), you fail to reject the null hypothesis, if it is smaller, you reject the null hypothesis, and then state what that means in the context of the problem.\n\n\nSingle Proportion\nWe conduct inference on a population proportion \\(\\pi\\) relative to a hypothesized value \\(\\pi_0\\). The test statistic is:\n\\[\nz = \\frac{\\hat{p} - \\pi}{\\sqrt{\\frac{\\pi(1 - \\pi)}{n}}}\n\\]\n\nExample: Test if more than 40% of diamonds are “Ideal” cut\n\n# Null hypothesis: p = 0.20\npi &lt;- 0.4\nn &lt;- nrow(diamonds)\nphat &lt;- mean(diamonds$cut == \"Ideal\")\n\n# Test statistic and p-value\nz &lt;- (phat - pi) / sqrt(pi * (1 - pi) / n)\np_value &lt;- 1 - pnorm(z)\n\ncat(\"Z-statistic:\", round(z, 3), \"\\n\")\n\nZ-statistic: -0.22 \n\ncat(\"P-value:\", round(p_value, 4))\n\nP-value: 0.587\n\n\n\n\n\n\nSingle Mean\nWe conduct inference on a population mean \\(\\mu\\) relative to a hypothesized value \\(\\mu_0\\). The test statistic is:\n\\[\nt = \\frac{\\bar{x} - \\mu_0}{s / \\sqrt{n}}\n\\]\n\nExample: Test if the average diamond price is $4000.\n\nmu0 &lt;- 4000\nx_bar &lt;- mean(diamonds$price)\ns &lt;- sd(diamonds$price)\nn &lt;- length(diamonds$price)\n\n# Test statistic and p-value\nt_stat &lt;- (x_bar - mu0) / (s / sqrt(n))\np_value &lt;- 2*(1 - pt(abs(t_stat), df = n - 1))\n\ncat(\"T-statistic:\", round(t_stat, 3), \"\\n\")\n\nT-statistic: -3.912 \n\ncat(\"P-value:\", round(p_value, 4))\n\nP-value: 1e-04\n\n\n\n\n\nExperimental Design: Power, Type I / Type II Error\nIn any hypothesis test, we face the possibility of making incorrect conclusions. These are formalized through Type I and Type II errors:\n\nType I Error (\\(\\alpha\\)): Rejecting the null hypothesis when it is actually true. This is controlled by the significance level of the test, often set to \\(\\alpha = 0.05\\).\nType II Error (\\(\\beta\\)): Failing to reject the null hypothesis when the alternative is actually true. This is harder to control and depends on the true parameter, sample size, and variance.\nPower of the Test: The probability of correctly rejecting the null hypothesis when the alternative is true: \\[\n\\text{Power} = 1 - \\beta\n\\]\n\nA powerful test detects meaningful effects and minimizes Type II error. Power increases when: - Sample size increases (\\(n \\uparrow\\)) - Effect size increases (true parameter is farther from null) - Variability decreases (standard deviation \\(\\downarrow\\)) - Significance level \\(\\alpha\\) increases (easier to reject null)\n\n\n\n\n\n\n\n\n\n\n\nThis diagram shows: - The blue curve is the null distribution (centered at 0). - The red dashed curve is the alternative distribution (shifted mean). - The dotted vertical line is the critical value (e.g., \\(z = 1.645\\) for \\(\\alpha = 0.05\\) in a one-sided test). - Area to the right of this cutoff under the null curve is \\(\\alpha\\). - Area to the left of this cutoff under the alternative curve is \\(\\beta\\). - The remaining area under the red curve (right tail) is power."
  },
  {
    "objectID": "course_guide_backup.html#two-sample-hypothesis-testing",
    "href": "course_guide_backup.html#two-sample-hypothesis-testing",
    "title": "MA206: Introduction to Probability and Statistics",
    "section": "Two Sample Hypothesis Testing",
    "text": "Two Sample Hypothesis Testing\n\nDifference of Proportions\nWe compare two population proportions to determine whether there is a significant difference between them. The test statistic is:\n\\[\nz = \\frac{\\hat{p}_1 - \\hat{p}_2}{\\sqrt{\\hat{p}(1 - \\hat{p}) \\left( \\frac{1}{n_1} + \\frac{1}{n_2} \\right)}}\n\\]\nWhere \\(\\hat{p}\\) is the pooled proportion.\n\nExample: Are “Ideal” cuts more common among Premium vs. Good color grades?\n\n# Subset to just Premium and Good clarity levels\ndf &lt;- diamonds %&gt;% filter(color ==\"D\" | color==\"E\")\n\ntable(df$cut, df$color)\n\n           \n               D    E    F    G    H    I    J\n  Fair       163  224    0    0    0    0    0\n  Good       662  933    0    0    0    0    0\n  Very Good 1513 2400    0    0    0    0    0\n  Premium   1603 2337    0    0    0    0    0\n  Ideal     2834 3903    0    0    0    0    0\n\n# Create a binary outcome: Ideal or not\ndf &lt;- df %&gt;%\n  mutate(is_ideal = cut == \"Ideal\")\n\n# Proportions\np1 &lt;- mean(df$is_ideal[df$color == \"D\"])\np2 &lt;- mean(df$is_ideal[df$color == \"E\"])\nn1 &lt;- sum(df$color == \"D\")\nn2 &lt;- sum(df$color == \"E\")\n\n# Pooled proportion\nphat &lt;- (p1 * n1 + p2 * n2) / (n1 + n2)\n\n# Test statistic\nz &lt;- (p1 - p2) / sqrt(phat * (1 - phat) * (1 / n1 + 1 / n2))\np_value &lt;- 2 * (1 - pnorm(abs(z)))\n\ncat(\"Z-statistic:\", round(z, 3), \"\\n\")\n\nZ-statistic: 2.566 \n\ncat(\"P-value:\", round(p_value, 4))\n\nP-value: 0.0103\n\n\n\n\n\n\nMultiple Proportions (Chi-Square Test of Independence)\nUsed when comparing proportions across more than two groups.\n\nExample: Is cut independent of color?\n\ntbl &lt;- table(diamonds$cut, diamonds$color)\nchisq.test(tbl)\n\n\n    Pearson's Chi-squared test\n\ndata:  tbl\nX-squared = 310.32, df = 24, p-value &lt; 2.2e-16\n\n\nThis test checks whether the distribution of cut types is independent of the diamond color. A small p-value suggests a dependency.\n\n\n\n\nDifference of Means\nUsed to compare two independent sample means.\n\nExample: Is the average price different between “Ideal” and “Fair” cuts?\n\ndf &lt;- diamonds %&gt;% filter(cut %in% c(\"Ideal\", \"Fair\"))\n\nt.test(price ~ cut, data = df)\n\n\n    Welch Two Sample t-test\n\ndata:  price by cut\nt = 9.7484, df = 1894.8, p-value &lt; 2.2e-16\nalternative hypothesis: true difference in means between group Fair and group Ideal is not equal to 0\n95 percent confidence interval:\n  719.9065 1082.5251\nsample estimates:\n mean in group Fair mean in group Ideal \n           4358.758            3457.542 \n\n\nThis performs a two-sample t-test, assuming unequal variances by default. The null hypothesis is that the means are equal.\n\n\n\n\nPaired Data\nIn paired designs, each observation in one group is paired with a related observation in the other. Since diamonds has no natural pairing, we’ll simulate a paired example.\n\nExample (Simulated): Price before and after resizing a set of diamonds\n\nset.seed(123)\n\n# Simulate paired prices: original and discounted\nn &lt;- 100\noriginal_price &lt;- sample(diamonds$price, n)\ndiscounted_price &lt;- original_price * runif(n, 0.85, 0.95)\n\nt.test(original_price, discounted_price, paired = TRUE)\n\n\n    Paired t-test\n\ndata:  original_price and discounted_price\nt = 8.9496, df = 99, p-value = 2.132e-14\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n 342.2012 537.1645\nsample estimates:\nmean difference \n       439.6829 \n\n\nThis tests whether the mean price before and after a simulated discount differs significantly."
  },
  {
    "objectID": "course_guide_backup.html#regression",
    "href": "course_guide_backup.html#regression",
    "title": "MA206: Introduction to Probability and Statistics",
    "section": "Regression",
    "text": "Regression\nIn this section we continue our multivariate inference with creating models for the purpose of identifying significance between explanatory (predictor) variables and the response. The function used to create the model, \\(\\hat{y_i}=f(x_i)\\) will make predictions, known as fitted values. Do to the variability in the data, these fitted values will not exactly predict the response (ie. \\(y_i \\neq \\hat{y}\\)) for all values in the response. These errors are the deviations from the response and the fitted values and are referred as residuals, with notation \\(\\epsilon_i = y_i - \\hat{y}_i\\).\nTo assess how well a model performs, the residuals are summarized in a few different methods:\n\nMean Absolute Deviation\nThe average magnitude of the residuals:\n\\[\nMAD = \\frac{1}{n}\\sum_{i=1}^n |y_i - \\hat{y}_i|\n\\]\n\n\nMean Squared Error:\nThe average magnitude of the residual-squared.\n\\[\nMSE = \\frac{1}{n}\\sum_{i=1}^n (y_i - \\hat{y}_i)^2\n\\]\n\n\nSimple Linear Regression\nThe above metrics could be applied to any model, however the central method to assess a linear relationship between two quantitative variables isSimple Linear Regression, or better known as the line of best fit:\n\\[\n\\hat{y} = \\beta_0 + \\beta_1 x\n\\]\nThe \\(\\beta\\)’s are the parameters of the model: the y-intercept and slope. The reason the method is the best fit is because we optimizes the choices for these two parameters by minimizing the sum of squared error:\n\\[\n\\begin{align}\nSSE &= \\sum_{i=1}^n (\\epsilon_i)^2 \\\\\n    &= \\sum_{i=1}^n (y_i - \\widehat{y}_i)^2 \\\\\n    &= \\sum_{i=1}^n (y_i - \\beta_0 - \\beta_1 x_i)^2 \\\\\n\\frac{\\partial}{\\partial \\beta_0}SSE &= \\frac{\\partial}{\\partial \\beta_0} \\sum_{i=1}^n (y_i - \\beta_0 - \\beta_1x_i)^2 \\\\\n0 &= -2\\sum_{i=1}^n (y_i - \\beta_0 - \\beta_1x_i) \\\\\n\\widehat{\\beta}_0 &= \\bar{y} - \\widehat{\\beta}_1 \\bar{x} \\\\\n\\frac{\\partial}{\\partial \\beta_1}SSE &= \\frac{\\partial}{\\partial \\beta_1} \\sum_{i=1}^n (y_i - \\beta_0 - \\beta_1x_i)^2 \\\\\n&= \\frac{\\partial}{\\partial \\beta_1} \\sum_{i=1}^n (y_i - (\\bar{y} - \\widehat{\\beta}_1 \\bar{x}) - \\widehat{\\beta}_1x_i)^2 \\\\\n0 &= -2\\sum_{i=1}^n (\\bar{x} - x_i)(y_i - \\bar{y} + \\widehat{\\beta}_1( \\bar{x} - x_i)) \\\\\n\\widehat{\\beta}_1 \\sum_{i=1}^n (x_i - \\bar{x})^2 &= \\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y}) \\\\\n\\widehat{\\beta}_1 &= \\frac{\\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^n (x_i - \\bar{x})^2} = \\frac{\\mathrm{Cov}(X,Y)}{\\mathrm{Var}(X)}\n\\end{align}\n\\]\nThe above is beyond the scope of this course, however it warrants a healthy appreciation for finding the line of best fit!\nIn practice, from the diamonds dataset we could model price as a function of carat:\n\n# Sample and fit model\nset.seed(206)\ndf &lt;- ggplot2::diamonds %&gt;% sample_n(1000)\nlm_simple &lt;- lm(price ~ carat, data = df)\nsummary(lm_simple)\n\n\nCall:\nlm(formula = price ~ carat, data = df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8077.3  -813.1    10.2   607.5  8759.7 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -2376.64      93.40  -25.45   &lt;2e-16 ***\ncarat        7885.65      99.37   79.35   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1499 on 998 degrees of freedom\nMultiple R-squared:  0.8632,    Adjusted R-squared:  0.8631 \nF-statistic:  6297 on 1 and 998 DF,  p-value: &lt; 2.2e-16\n\n\nThere is a lot in the summary output, however the main ideas here lie in the magnitude and sign of the coefficient, looking for practical significance, and also looking at the size of the p-value relative to a chosen \\(\\alpha\\), checking for statistical significance.\nYou may be wondering in a line of best fit, where did the p-value come from? Good question! In addition to finding the line of best fit, our linear model assesses the relevance of all parameters in the model. This assessment is a *one-sample t-test for every ! If there is significance, then there is a significant assocation between the explanatory variable and the response.\n\n\n\n\n\n\n\n\n\n\n\n\nMultiple Linear Regression\nWe can extend linear regression to in fact include as many predictor variables as we want (as long as we have more observations than variables!). This is implemented through Multiple Linear Regression:\n\\[\n\\widehat{y} = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\dots + \\beta_p x_p\n\\]\nThe derivation would be too lengthy to do them individually previously, however through Matrix Algebra (MA371 anyone?!), the solved vector of coefficients, \\(\\widehat{\\beta}_{p\\times1}\\) has the following solution:\n\\[\n\\widehat\\beta_{p\\times 1} = (X_{p\\times n}^TX_{n \\times p})^{-1}X_{p \\times n}^T \\vec{y}_{n \\times 1}\n\\] Observe the subscripts for the dimensions of the matrices, ending with a \\(p \\times 1\\) vector for the coefficients that minimize the SSE.\nHere, we use carat(Q), depth(Q), table(Q), and color(Q) to predict price.\n\n\n\n\n\n\nNote\n\n\n\nWhen a categorical variable like color is included in a regression model, R converts it into multiple indicator (dummy) variables, one for each level except the reference level (usually the first alphabetically, unless manually changed).\nEach dummy variable shifts the intercept of the regression line for that level, while the slope(s) for the numeric predictors remain the same. This means:\n\nR is effectively fitting a separate line of best fit for each level of the categorical variable — all with the same slope, but different intercepts.\n\nFor example, including color in lm(price ~ carat + depth + table + color) produces:\n\nOne baseline line (intercept) for the reference group (e.g., color = \"D\").\nAdditional parallel lines for each other color group (e.g., E, F, G, etc.), each shifted vertically by its corresponding coefficient (e.g., colorE, colorF, etc.).\n\nSo if the coefficient for colorE is -500, then diamonds with color E are estimated to be $500 less expensive than D-colored diamonds at the same carat, depth, and table values.\nThis approach captures group differences in the starting point (intercepts) of the response, while assuming the effect of continuous predictors (e.g. carat) is constant across all groups.\n\n\n\nlm_multi &lt;- lm(price ~ carat + depth + table + color, data = df)\nsummary(lm_multi)\n\n\nCall:\nlm(formula = price ~ carat + depth + table + color, data = df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-9095.0  -787.8   -80.5   568.3  7873.0 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 10739.61    2692.30   3.989 7.12e-05 ***\ncarat        8251.22      97.16  84.920  &lt; 2e-16 ***\ndepth        -112.34      33.40  -3.364 0.000799 ***\ntable        -115.68      20.77  -5.569 3.30e-08 ***\ncolor.L     -1555.67     146.23 -10.639  &lt; 2e-16 ***\ncolor.Q      -843.07     133.77  -6.303 4.40e-10 ***\ncolor.C       -85.92     129.08  -0.666 0.505778    \ncolor^4        75.10     117.70   0.638 0.523580    \ncolor^5      -123.89     116.66  -1.062 0.288500    \ncolor^6       -72.64     104.92  -0.692 0.488903    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1385 on 990 degrees of freedom\nMultiple R-squared:  0.8842,    Adjusted R-squared:  0.8832 \nF-statistic: 840.1 on 9 and 990 DF,  p-value: &lt; 2.2e-16\n\n\nYou can compare \\(R^2\\) values and p-values to determine whether the additional variables meaningfully improve the model.\n\nAdding Interaction Terms\nWhat if the effect of one variable depends on another? For example, maybe the impact of carat on price differs across color levels. In that case, we can include an interaction term in the model:\n\nlm_interact &lt;- lm(price ~ carat * color, data = df)\nsummary(lm_interact)\n\n\nCall:\nlm(formula = price ~ carat * color, data = df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-9236.8  -767.5   -13.5   648.9  8199.4 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   -2642.59      99.53 -26.551  &lt; 2e-16 ***\ncarat          8119.05     103.78  78.236  &lt; 2e-16 ***\ncolor.L        -132.35     306.56  -0.432 0.666033    \ncolor.Q         315.80     290.51   1.087 0.277286    \ncolor.C         -16.57     275.23  -0.060 0.952018    \ncolor^4         377.23     245.62   1.536 0.124911    \ncolor^5         593.39     239.84   2.474 0.013526 *  \ncolor^6        -225.89     209.92  -1.076 0.282167    \ncarat:color.L -1533.77     313.82  -4.887 1.19e-06 ***\ncarat:color.Q -1137.53     294.94  -3.857 0.000122 ***\ncarat:color.C   110.35     287.00   0.384 0.700695    \ncarat:color^4  -311.72     262.03  -1.190 0.234479    \ncarat:color^5  -865.55     257.40  -3.363 0.000802 ***\ncarat:color^6   255.92     222.64   1.149 0.250641    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1375 on 986 degrees of freedom\nMultiple R-squared:  0.8864,    Adjusted R-squared:  0.8849 \nF-statistic: 591.9 on 13 and 986 DF,  p-value: &lt; 2.2e-16\n\n\n\n\n\n\n\n\nTip\n\n\n\n\nEach level of the color factor has its own intercept and its own slope for carat.\nThe reference group (e.g., color = “D”) uses the coefficient for carat directly:\n\n\\[ \\text{Slope}_D = \\beta_{carat} \\]\n\nFor other groups, the slope is:\n\n\\[ \\text{Slope}_{\\text{color}} = \\beta_{carat} + \\beta_{\\text{carat:color}} \\]\nExample: If carat has a coefficient of 8000 and carat:colorE has a coefficient of -1000, then for color E diamonds:\n\nIntercept = Intercept + \\(\\beta_{\\text{colorE}}\\)\nSlope = $8000 - 1000 = $7000\n\nSo carat still increases price for color E diamonds, but not as sharply as it does for the reference group (D).\n\n\nVisualizing Interactions\n\nYou can visualize the effect of interaction terms by plotting separate regression lines for each group:\n\nggplot(df, aes(x = carat, y = price, color = color)) +\n  geom_point(alpha = 0.5) +\n  geom_smooth(method = \"lm\", formula = y ~ x, se = FALSE) +\n  labs(\n    title = \"Separate Regression Lines by Diamond Color\",\n    x = \"Carat\",\n    y = \"Price (USD)\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nNote: Each color group has its own slope and intercept, as determined by the interaction model.\n\n\n\n\n\n\nNote\n\n\n\nMultiple linear regression allows us to model relationships between one response variable and many predictors.\nCategorical variables create parallel lines (same slope, different intercepts) unless interaction terms are added.\nInteraction terms create separate lines of best fit (different slopes and intercepts) for each group.\n\n\n\n\n\n\nGoodness of Fit\nWe use several metrics to assess the quality of a regression model:\n\n\\(R^2\\): Proportion of variance in the response explained by the predictors.\nResidual Standard Error (RSE): Average size of the residuals.\nF-statistic: Overall significance of the regression.\nResidual Plots: Visual diagnostics to assess assumptions.\n\n\n# Residual plot\nplot(lm_multi, which = 1)  # Residuals vs Fitted\n\n\n\n\n\n\n\n\n\n# Histogram of residuals\nresiduals &lt;- resid(lm_multi)\nhist(residuals, breaks = 30, col = \"lightblue\", main = \"Histogram of Residuals\", xlab = \"Residual\")\n\n\n\n\n\n\n\n\nWe want residuals to be roughly normally distributed and randomly scattered around zero to satisfy assumptions of linear regression.\n\n\nANOVA\nAnalysis of Variance (ANOVA) is a statistical method used to compare the means of three or more groups to determine if at least one of the group means is significantly different from the others.\n\n\nTheoretical Foundation\nANOVA works by partitioning the total variability in the data into two components: - Between-group variability: how much the group means differ from the overall mean. - Within-group variability: how much individual observations vary within each group.\nThe core idea is that if the between-group variability is large relative to the within-group variability, then at least one group mean is likely different.\n[ F = = ]\nWhere: - ( SSB ) = Sum of Squares Between - ( SSW ) = Sum of Squares Within - ( k ) = number of groups - ( n ) = total number of observations\nIf the calculated F-statistic is large, and the p-value is small (typically &lt; 0.05), we reject the null hypothesis:\n\n( H_0: _1 = _2 = = _k ) (all group means are equal)\n( H_A: ) At least one group mean is different\n\n\n\n\nExample: Do Different Diamond Cuts Have Different Average Prices?\nWe’ll use the diamonds dataset and compare the mean price across the five levels of the cut variable.\n\n# Sample for speed\nset.seed(206)\ndf &lt;- diamonds %&gt;% sample_n(1000)\n\n# Summary statistics by cut\ndf %&gt;%\n  group_by(cut) %&gt;%\n  summarise(mean_price = mean(price), n = n())\n\n# A tibble: 5 × 3\n  cut       mean_price     n\n  &lt;ord&gt;          &lt;dbl&gt; &lt;int&gt;\n1 Fair           3677.    28\n2 Good           3429.    74\n3 Very Good      4561.   226\n4 Premium        4355.   273\n5 Ideal          3590.   399\n\n\n\n\n\nRun ANOVA Test\n\n# One-way ANOVA: price ~ cut\nanova_model &lt;- aov(price ~ cut, data = df)\nsummary(anova_model)\n\n             Df    Sum Sq  Mean Sq F value Pr(&gt;F)  \ncut           4 1.996e+08 49901236   3.065  0.016 *\nResiduals   995 1.620e+10 16283224                 \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThis will output the F-statistic and p-value. A small p-value (e.g. &lt; 0.05) suggests that at least one cut has a significantly different mean price.\n\n\n\n\nVisualize the Group Differences\n\nggplot(df, aes(x = cut, y = price)) +\n  geom_boxplot(fill = \"lightblue\") +\n  labs(title = \"Boxplot of Diamond Price by Cut\",\n       x = \"Cut\", y = \"Price ($)\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nBoxplots help visualize both the median and spread of price within each cut level.\n\n\n\nFollow-Up: Which Cuts Are Different?\nIf the ANOVA result is significant, we can follow up with a Tukey HSD test to identify which group pairs differ.\n\nTukeyHSD(anova_model)\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = price ~ cut, data = df)\n\n$cut\n                        diff        lwr        upr     p adj\nGood-Fair         -247.90830 -2694.6097 2198.79307 0.9987125\nVery Good-Fair     884.55594 -1324.7679 3093.87977 0.8096253\nPremium-Fair       678.19322 -1510.0655 2866.45192 0.9157755\nIdeal-Fair         -86.59085 -2242.4691 2069.28738 0.9999672\nVery Good-Good    1132.46424  -344.4875 2609.41597 0.2227928\nPremium-Good       926.10152  -519.1496 2371.35262 0.4030816\nIdeal-Good         161.31745 -1234.4209 1557.05582 0.9978434\nPremium-Very Good -206.36272 -1198.0859  785.36051 0.9795621\nIdeal-Very Good   -971.14679 -1889.2153  -53.07827 0.0320166\nIdeal-Premium     -764.78408 -1630.9331  101.36495 0.1126355\n\n\nThis test controls the family-wise error rate and gives pairwise confidence intervals and p-values.\n\n\n\nInterpretation\n\nIf p &lt; 0.05 in the ANOVA, we conclude that at least one cut differs in mean price.\nUse TukeyHSD to find out which cuts are significantly different.\nANOVA assumes:\n\nIndependent observations\nNormally distributed residuals\nEqual variances across groups (can check with Levene’s test)"
  },
  {
    "objectID": "code_annex.html",
    "href": "code_annex.html",
    "title": "MA206: Code Annex",
    "section": "",
    "text": "Adding Dependencies\nIn RStudio we use a common framework called tidyverse which holds a plethora of useful functions to work with and manipulate data. The other packages are not always necessary, only add the libraries you need at the beginning of your code document. If you do not have that library installed, in the console tab in the bottom-left pane of your RStudio use the function install.packages('the_package_you_want_to_install') and RStudio will install it for you.\n\n\nData Sampling\n\nset.seed(1991)\ndata(diamonds)\ndf &lt;- diamonds %&gt;% sample_n(size = 1000)\n\n\n\nTable Preview\n\ndf %&gt;% head() %&gt;%\n  kable(\"html\", align = \"c\") %&gt;%\n  kable_styling(\n    full_width = FALSE,\n    position = \"center\",\n    bootstrap_options = c(\"striped\", \"hover\", \"condensed\", \"bordered\")\n  )\n\n\n\n\ncarat\ncut\ncolor\nclarity\ndepth\ntable\nprice\nx\ny\nz\n\n\n\n\n1.02\nGood\nG\nVS2\n63.8\n59.0\n6080\n6.34\n6.27\n4.02\n\n\n0.31\nIdeal\nF\nVVS1\n61.9\n53.5\n882\n4.36\n4.39\n2.71\n\n\n0.60\nPremium\nD\nSI2\n61.3\n61.0\n1428\n5.46\n5.40\n3.33\n\n\n0.41\nIdeal\nE\nIF\n62.1\n54.0\n1419\n4.75\n4.81\n2.97\n\n\n0.72\nVery Good\nH\nVS1\n62.2\n54.0\n2877\n5.74\n5.76\n3.57\n\n\n1.20\nIdeal\nF\nVS2\n62.6\n56.0\n8486\n6.78\n6.73\n4.23\n\n\n\n\n\n\n\nHistograms\n\np1 &lt;- df %&gt;% ggplot(aes(x = carat)) + geom_histogram(bins = 30) + theme_minimal()\np2 &lt;- df %&gt;% ggplot(aes(x = depth)) + geom_histogram(bins = 30) + theme_minimal()\np3 &lt;- df %&gt;% ggplot(aes(x = price)) + geom_histogram(bins = 30) + theme_minimal()\np1 | p2 | p3\n\n\n\n\n\n\n\n\n\n\nBoxplots\n\np4 &lt;- df %&gt;% ggplot(aes(x = carat)) + geom_boxplot() + theme_minimal()\np5 &lt;- df %&gt;% ggplot(aes(x = depth)) + geom_boxplot() + theme_minimal()\np6 &lt;- df %&gt;% ggplot(aes(x = price)) + geom_boxplot() + theme_minimal()\np4 | p5 | p6\n\n\n\n\n\n\n\n\n\n\nSummary Statistics Table\n\ndf %&gt;%\n  summarise(\n    across(\n      where(is.numeric),\n      list(\n        Mean = ~mean(.x, na.rm = TRUE),\n        Median = ~median(.x, na.rm = TRUE),\n        SD = ~sd(.x, na.rm = TRUE),\n        Var = ~var(.x, na.rm = TRUE),\n        Min = ~min(.x, na.rm = TRUE),\n        Max = ~max(.x, na.rm = TRUE)\n      ),\n      .names = \"{.col}_{.fn}\"\n    )\n  ) %&gt;%\n  round(2) %&gt;%\n  pivot_longer(everything(), names_to = c(\"Variable\", \"Stat\"), names_sep = \"_\") %&gt;%\n  pivot_wider(names_from = Stat, values_from = value) %&gt;%\n  kable(\"html\", align = \"c\") %&gt;%\n  kable_styling(\n    full_width = FALSE,\n    position = \"center\",\n    bootstrap_options = c(\"striped\", \"hover\", \"condensed\", \"bordered\")\n  )\n\n\n\n\nVariable\nMean\nMedian\nSD\nVar\nMin\nMax\n\n\n\n\ncarat\n0.79\n0.70\n0.46\n0.21\n0.23\n3.24\n\n\ndepth\n61.77\n61.90\n1.40\n1.97\n56.30\n68.90\n\n\ntable\n57.48\n57.00\n2.26\n5.11\n50.00\n66.00\n\n\nprice\n3873.55\n2387.00\n3912.55\n15308046.06\n337.00\n18692.00\n\n\nx\n5.72\n5.68\n1.10\n1.21\n3.88\n9.44\n\n\ny\n5.72\n5.68\n1.09\n1.20\n3.90\n9.40\n\n\nz\n3.53\n3.52\n0.68\n0.46\n2.39\n5.85\n\n\n\n\n\n\n\nScatterplots\n\np7 &lt;- df %&gt;% ggplot(aes(x = carat, y = price, color = clarity)) + geom_point() + theme_minimal()\np8 &lt;- df %&gt;% ggplot(aes(x = table, y = price, color = cut)) + geom_point() + theme_minimal()\np9 &lt;- df %&gt;% ggplot(aes(x = depth, y = price, color = color)) + geom_point() + theme_minimal()\np7 | p8 | p9\n\n\n\n\n\n\n\n\n\n\nBar Charts\n\np10 &lt;- df %&gt;% group_by(cut) %&gt;% summarise(avg_price = mean(price)) %&gt;%\n  ggplot(aes(x = cut, y = avg_price)) +\n  geom_bar(stat = \"identity\", fill = \"pink\") +\n  labs(title = \"Average Price by Diamond Cut\", x = \"Cut\", y = \"Avg Price\") +\n  theme_minimal()\n\np11 &lt;- df %&gt;% group_by(color) %&gt;% summarise(avg_price = mean(price)) %&gt;%\n  ggplot(aes(x = color, y = avg_price)) +\n  geom_bar(stat = \"identity\", fill = \"skyblue\") +\n  labs(title = \"Average Price by Diamond Color\", x = \"Color\", y = \"Avg Price\") +\n  theme_minimal()\n\np12 &lt;- df %&gt;% group_by(clarity) %&gt;% summarise(avg_price = mean(price)) %&gt;%\n  ggplot(aes(x = clarity, y = avg_price)) +\n  geom_bar(stat = \"identity\", fill = \"magenta\") +\n  labs(title = \"Average Price by Diamond Clarity\", x = \"Clarity\", y = \"Avg Price\") +\n  theme_minimal()\n\np10 | p11 | p12\n\n\n\n\n\n\n\n\n\n\nCorrelation Table\n\ndf %&gt;%\n  select(where(is.numeric)) %&gt;%\n  cor(use = \"pairwise.complete.obs\") %&gt;%\n  round(2) %&gt;%\n  kable(\"html\", align = \"c\") %&gt;%\n  kable_styling(\n    full_width = FALSE,\n    position = \"center\",\n    bootstrap_options = c(\"striped\", \"hover\", \"condensed\", \"bordered\")\n  )\n\n\n\n\n\ncarat\ndepth\ntable\nprice\nx\ny\nz\n\n\n\n\ncarat\n1.00\n0.00\n0.22\n0.91\n0.98\n0.98\n0.98\n\n\ndepth\n0.00\n1.00\n-0.32\n-0.02\n-0.06\n-0.07\n0.06\n\n\ntable\n0.22\n-0.32\n1.00\n0.17\n0.24\n0.24\n0.20\n\n\nprice\n0.91\n-0.02\n0.17\n1.00\n0.88\n0.88\n0.88\n\n\nx\n0.98\n-0.06\n0.24\n0.88\n1.00\n1.00\n0.99\n\n\ny\n0.98\n-0.07\n0.24\n0.88\n1.00\n1.00\n0.99\n\n\nz\n0.98\n0.06\n0.20\n0.88\n0.99\n0.99\n1.00\n\n\n\n\n\n\n\nProportion Test Example\n\npi &lt;- 0.4\nn &lt;- nrow(diamonds)\nphat &lt;- mean(diamonds$cut == \"Ideal\")\nz &lt;- (phat - pi) / sqrt(pi * (1 - pi) / n)\np_value &lt;- 1 - pnorm(z)\ncat(\"Z-statistic: \", round(z, 3), \"\\nP-value: \", round(p_value, 4))\n\nZ-statistic:  -0.22 \nP-value:  0.587\n\n\n\n\nMean Test Example\n\nmu0 &lt;- 4000\nx_bar &lt;- mean(diamonds$price)\ns &lt;- sd(diamonds$price)\nn &lt;- length(diamonds$price)\nt_stat &lt;- (x_bar - mu0) / (s / sqrt(n))\np_value &lt;- 2 * (1 - pt(abs(t_stat), df = n - 1))\ncat(\"T-statistic: \", round(t_stat, 3), \"\\nP-value: \", round(p_value, 4))\n\nT-statistic:  -3.912 \nP-value:  1e-04\n\n\n\n\nTwo-Sample Proportion Test\n\ndf_prop &lt;- diamonds %&gt;% filter(color %in% c(\"D\", \"E\")) %&gt;% mutate(is_ideal = cut == \"Ideal\")\np1 &lt;- mean(df_prop$is_ideal[df_prop$color == \"D\"])\np2 &lt;- mean(df_prop$is_ideal[df_prop$color == \"E\"])\nn1 &lt;- sum(df_prop$color == \"D\")\nn2 &lt;- sum(df_prop$color == \"E\")\nphat_pool &lt;- (p1 * n1 + p2 * n2) / (n1 + n2)\nz &lt;- (p1 - p2) / sqrt(phat_pool * (1 - phat_pool) * (1 / n1 + 1 / n2))\np_value &lt;- 2 * (1 - pnorm(abs(z)))\ncat(\"Z-statistic: \", round(z, 3), \"\\nP-value: \", round(p_value, 4))\n\nZ-statistic:  2.566 \nP-value:  0.0103\n\n\n\n\nChi-Square Test\n\ntbl &lt;- table(diamonds$cut, diamonds$color)\nchisq.test(tbl)\n\n\n    Pearson's Chi-squared test\n\ndata:  tbl\nX-squared = 310.32, df = 24, p-value &lt; 2.2e-16\n\n\n\n\nIndependent T-Test\n\ndf_t &lt;- diamonds %&gt;% filter(cut %in% c(\"Ideal\", \"Fair\"))\nt.test(price ~ cut, data = df_t)\n\n\n    Welch Two Sample t-test\n\ndata:  price by cut\nt = 9.7484, df = 1894.8, p-value &lt; 2.2e-16\nalternative hypothesis: true difference in means between group Fair and group Ideal is not equal to 0\n95 percent confidence interval:\n  719.9065 1082.5251\nsample estimates:\n mean in group Fair mean in group Ideal \n           4358.758            3457.542 \n\n\n\n\nPaired T-Test (Simulated)\n\nset.seed(123)\nn &lt;- 100\norig &lt;- sample(diamonds$price, n)\ndisc &lt;- orig * runif(n, 0.85, 0.95)\nt.test(orig, disc, paired = TRUE)\n\n\n    Paired t-test\n\ndata:  orig and disc\nt = 8.9496, df = 99, p-value = 2.132e-14\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n 342.2012 537.1645\nsample estimates:\nmean difference \n       439.6829 \n\n\n\n\nSimple Linear Regression\n\nset.seed(206)\ndf_lm &lt;- diamonds %&gt;% sample_n(1000)\nlm_simple &lt;- lm(price ~ carat, data = df_lm)\nsummary(lm_simple)\n\n\nCall:\nlm(formula = price ~ carat, data = df_lm)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8077.3  -813.1    10.2   607.5  8759.7 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -2376.64      93.40  -25.45   &lt;2e-16 ***\ncarat        7885.65      99.37   79.35   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1499 on 998 degrees of freedom\nMultiple R-squared:  0.8632,    Adjusted R-squared:  0.8631 \nF-statistic:  6297 on 1 and 998 DF,  p-value: &lt; 2.2e-16\n\n\n\n\nMultiple Linear Regression\n\nlm_multi &lt;- lm(price ~ carat + depth + table, data = df_lm)\nsummary(lm_multi)\n\n\nCall:\nlm(formula = price ~ carat + depth + table, data = df_lm)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8252.9  -784.5   -22.4   624.7  8211.9 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 13392.24    2845.47   4.707 2.88e-06 ***\ncarat        7965.09      99.01  80.445  &lt; 2e-16 ***\ndepth        -146.58      35.32  -4.149 3.62e-05 ***\ntable        -118.01      22.05  -5.351 1.09e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1475 on 996 degrees of freedom\nMultiple R-squared:  0.8679,    Adjusted R-squared:  0.8675 \nF-statistic:  2181 on 3 and 996 DF,  p-value: &lt; 2.2e-16\n\n\n\n\nRegression Diagnostics\n\nplot(lm_multi, which = 1)\n\n\n\n\n\n\n\n\n\nresid &lt;- resid(lm_multi)\nhist(resid, breaks = 30, main = \"Histogram of Residuals\", xlab = \"Residual\")\n\n\n\n\n\n\n\n\n\n\nANOVA\n\nanova_model &lt;- aov(price ~ cut, data = df_lm)\nsummary(anova_model)\n\n             Df    Sum Sq  Mean Sq F value Pr(&gt;F)  \ncut           4 1.996e+08 49901236   3.065  0.016 *\nResiduals   995 1.620e+10 16283224                 \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\nTukey HSD\nTukeyHSD(anova_model)"
  },
  {
    "objectID": "fiberL.html",
    "href": "fiberL.html",
    "title": "fiberL: Computer Vision for Fiber Analysis",
    "section": "",
    "text": "fiberL is a specialized computer vision application designed to automatically analyze Scanning Electron Microscope (SEM) images of fiber networks. It performs sophisticated image processing, fiber detection, network reconstruction, and quantitative analysis to extract meaningful structural properties from complex fiber assemblies.\nRepository: github.com/lonespear/fiberL"
  },
  {
    "objectID": "fiberL.html#overview",
    "href": "fiberL.html#overview",
    "title": "fiberL: Computer Vision for Fiber Analysis",
    "section": "",
    "text": "fiberL is a specialized computer vision application designed to automatically analyze Scanning Electron Microscope (SEM) images of fiber networks. It performs sophisticated image processing, fiber detection, network reconstruction, and quantitative analysis to extract meaningful structural properties from complex fiber assemblies.\nRepository: github.com/lonespear/fiberL"
  },
  {
    "objectID": "fiberL.html#key-features",
    "href": "fiberL.html#key-features",
    "title": "fiberL: Computer Vision for Fiber Analysis",
    "section": "Key Features",
    "text": "Key Features\n\n1. Advanced Image Preprocessing\nThe application employs state-of-the-art image enhancement techniques:\n\nAnisotropic Diffusion Filtering: Noise reduction while preserving edges using Perona-Malik equations\nCoherence-Enhanced Diffusion: Follows fiber orientations for structure-aware smoothing\nMorphological Top-Hat Transform: Background removal and contrast enhancement\nMulti-Stage Thresholding: Binary thresholding combined with Canny edge detection\nSkeletonization: Extracts fiber centerlines for precise measurement\n\n# Example preprocessing workflow\ndef preprocess_image(image):\n    # Anisotropic diffusion\n    smoothed = anisotropic_diffusion(image, iterations=10, kappa=50)\n\n    # Coherence-enhanced diffusion\n    enhanced = coherence_diffusion(smoothed, alpha=0.01, iterations=5)\n\n    # Top-hat transform\n    tophat = morphology.white_tophat(enhanced, selem=morphology.disk(15))\n\n    # Thresholding\n    binary = threshold_otsu(tophat)\n\n    # Skeletonization\n    skeleton = morphology.skeletonize(binary)\n\n    return skeleton\n\n\n2. Intelligent Fiber Detection\nUses advanced morphological operations to identify fiber network topology:\n\nBranch Point Detection: 17 specialized kernels for T-junctions, Y-junctions, and crossings\nSmall Segment Pruning: Removes noise artifacts\nFiber Tip Detection: Identifies fiber endpoints\nHit-or-Miss Transforms: Pattern matching for junction types\n\n# Branch point detection kernels\nT_JUNCTION_KERNELS = [\n    np.array([[0,1,0], [1,1,1], [0,0,0]]),  # T-top\n    np.array([[0,0,0], [1,1,1], [0,1,0]]),  # T-bottom\n    # ... 15 more rotation variants\n]\n\n\n3. Network Reconstruction\nSophisticated graph-based algorithms reconstruct the complete fiber network:\n\nIntersection Association: Links fibers at branch points using cosine similarity\nEdge Connectivity Analysis: Determines fiber continuity through junctions\nTip-to-Tip Association: Connects broken fibers based on proximity and orientation\nCurvature-Aware Merging: Uses PCA to evaluate fiber curvature for intelligent merging\n\nAlgorithm:\ndef reconstruct_network(skeleton):\n    # 1. Detect branch points and tips\n    branch_points = detect_branches(skeleton)\n    tips = detect_tips(skeleton)\n\n    # 2. Extract fiber segments\n    segments = extract_segments(skeleton, branch_points)\n\n    # 3. Associate segments at intersections\n    for branch in branch_points:\n        edges = get_connected_edges(branch, segments)\n        # Merge edges with similar orientation (cosine similarity)\n        merged = merge_by_orientation(edges, threshold=0.9)\n\n    # 4. Connect broken fibers\n    for tip1, tip2 in itertools.combinations(tips, 2):\n        if should_connect(tip1, tip2):\n            connect_fibers(tip1, tip2)\n\n    return reconstructed_network\n\n\n4. Quantitative Analysis\nExtracts comprehensive quantitative metrics:\n\nFiber Length\n\nArc Length Measurement: Follows fiber curvature for accurate length\nCalibrated Units: Converts pixels to physical units (nm, μm, mm)\n\ndef calculate_fiber_length(fiber_coords, scale_factor):\n    \"\"\"\n    Calculate arc length of fiber\n\n    Parameters:\n    -----------\n    fiber_coords : array\n        (N, 2) array of fiber pixel coordinates\n    scale_factor : float\n        Microns per pixel\n\n    Returns:\n    --------\n    length : float\n        Fiber length in microns\n    \"\"\"\n    distances = np.sqrt(np.sum(np.diff(fiber_coords, axis=0)**2, axis=1))\n    length_pixels = np.sum(distances)\n    length_microns = length_pixels * scale_factor\n    return length_microns\n\n\nOrientation Analysis\n\nPCA-Based Directional Analysis: Principal component analysis for fiber direction\nAngle Distribution: 0-180° orientation mapping\nRose Diagrams: Circular histograms for orientation visualization\n\ndef calculate_fiber_orientation(fiber_coords):\n    \"\"\"\n    Calculate fiber orientation using PCA\n\n    Returns:\n    --------\n    angle : float\n        Orientation angle in degrees (0-180)\n    \"\"\"\n    pca = PCA(n_components=1)\n    pca.fit(fiber_coords)\n\n    # Get principal component direction\n    direction = pca.components_[0]\n    angle = np.arctan2(direction[1], direction[0])\n\n    # Convert to 0-180 range\n    angle_degrees = np.degrees(angle) % 180\n\n    return angle_degrees\n\n\nPersistence Length\n\nTangent Correlation Analysis: Measures fiber stiffness\nPolymer Physics Metric: Characterizes mechanical properties\n\ndef calculate_persistence_length(fiber_coords, scale_factor):\n    \"\"\"\n    Calculate persistence length via tangent correlation\n\n    Persistence length (Lp) characterizes the length scale\n    over which a fiber remains straight.\n\n    Formula: &lt;cos(θ(s))&gt; = exp(-s/Lp)\n    \"\"\"\n    # Calculate tangent vectors along fiber\n    tangents = np.diff(fiber_coords, axis=0)\n    tangents = tangents / np.linalg.norm(tangents, axis=1, keepdims=True)\n\n    # Calculate correlation as function of arc length\n    correlations = []\n    arc_lengths = []\n\n    for separation in range(1, len(tangents)):\n        cos_angles = np.sum(tangents[:-separation] * tangents[separation:], axis=1)\n        correlations.append(np.mean(cos_angles))\n        arc_lengths.append(separation * scale_factor)\n\n    # Fit exponential: &lt;cos(θ)&gt; = exp(-s/Lp)\n    def exponential(s, Lp):\n        return np.exp(-s / Lp)\n\n    popt, _ = curve_fit(exponential, arc_lengths, correlations)\n    persistence_length = popt[0]\n\n    return persistence_length\n\n\nLambda Coefficient\n\nStiffness Ratio: log(persistence length) / actual length\nDimensionless Metric: Compares intrinsic stiffness to fiber geometry\n\n\n\n\n5. Interactive Streamlit Interface\nUser-friendly web application with:\n\nScale Calibration Tool: Point-and-click distance measurement\nROI Cropping: Select regions of interest with preview\nParameter Tuning: 17+ adjustable processing parameters\n\nDiffusion iterations and constants\nThreshold values\nPruning lengths\nAssociation thresholds\n\nReal-Time Processing: Immediate visual feedback\nBatch Processing: Analyze multiple images\n\nInterface Workflow:\n1. Upload SEM Image\n   ↓\n2. Scale Calibration (optional)\n   - Click two points\n   - Enter known distance\n   ↓\n3. ROI Selection (optional)\n   - Draw bounding box\n   - Preview cropped region\n   ↓\n4. Parameter Adjustment\n   - Tune preprocessing\n   - Set detection thresholds\n   ↓\n5. Process & Analyze\n   - View color-coded network\n   - Export statistics\n\n\n6. Visualization Suite\nComprehensive visual output:\n\nColor-Coded Networks: Fibers colored by length or randomly\nLength Histograms: Distribution analysis\nRose Diagrams: Orientation visualization\nScatter Plots: Length vs. angle relationships\nMulti-Panel Summaries: Combined figure output\n\n# Example visualization code\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndef visualize_results(fibers_df):\n    fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n\n    # Length histogram\n    axes[0,0].hist(fibers_df['length'], bins=30, edgecolor='black')\n    axes[0,0].set_xlabel('Fiber Length (μm)')\n    axes[0,0].set_ylabel('Count')\n\n    # Rose diagram\n    plot_rose_diagram(fibers_df['orientation'], ax=axes[0,1])\n\n    # Length vs. Angle\n    axes[1,0].scatter(fibers_df['orientation'], fibers_df['length'])\n    axes[1,0].set_xlabel('Orientation (degrees)')\n    axes[1,0].set_ylabel('Length (μm)')\n\n    # Color-coded network\n    plot_fiber_network(fibers_df, ax=axes[1,1], color_by='length')\n\n    plt.tight_layout()\n    return fig"
  },
  {
    "objectID": "fiberL.html#technical-implementation",
    "href": "fiberL.html#technical-implementation",
    "title": "fiberL: Computer Vision for Fiber Analysis",
    "section": "Technical Implementation",
    "text": "Technical Implementation\n\nCore Technologies\n\n\n\n\n\n\n\n\nComponent\nTechnology\nPurpose\n\n\n\n\nImage Processing\nOpenCV, scikit-image\nMorphological operations, filtering\n\n\nNumerical Computing\nNumPy, SciPy\nArray operations, optimization\n\n\nMachine Learning\nscikit-learn\nPCA for orientation analysis\n\n\nVisualization\nMatplotlib, Seaborn\nStatistical plots\n\n\nWeb Interface\nStreamlit\nInteractive application\n\n\nData Management\nPandas\nStructured data handling\n\n\n\n\n\nSystem Architecture\n┌─────────────────────────────────────┐\n│         Streamlit UI                │\n│  (Parameter Input & Visualization)  │\n└──────────────┬──────────────────────┘\n               │\n               ↓\n┌─────────────────────────────────────┐\n│       fiberL Core Engine            │\n├─────────────────────────────────────┤\n│  • Image Preprocessing              │\n│  • Fiber Detection                  │\n│  • Network Reconstruction           │\n│  • Quantitative Analysis            │\n└──────────────┬──────────────────────┘\n               │\n               ↓\n┌─────────────────────────────────────┐\n│     Computer Vision Backends        │\n│  (OpenCV, scikit-image, NumPy)      │\n└─────────────────────────────────────┘\n\n\nProject Structure\nfiberL/\n├── fiberL.py                  # Core analysis engine (1,177 lines)\n│   ├── UnionFind class        # Disjoint set data structure\n│   ├── Image preprocessing    # Diffusion, morphology\n│   ├── Skeletonization        # Centerline extraction\n│   ├── fiberL class          # Main analyzer\n│   │   ├── detect_branches()\n│   │   ├── reconstruct_network()\n│   │   ├── calculate_metrics()\n│   │   └── visualize()\n│   └── Statistical analysis   # Summary statistics\n│\n├── app.py                     # Streamlit web interface\n│   ├── File upload\n│   ├── Scale calibration\n│   ├── ROI cropping\n│   ├── Parameter controls\n│   └── Results display\n│\n├── requirements.txt           # Python dependencies\n│   ├── opencv-python&gt;=4.5.0\n│   ├── scikit-image&gt;=0.18.0\n│   ├── streamlit&gt;=1.10.0\n│   ├── numpy&gt;=1.20.0\n│   ├── scipy&gt;=1.7.0\n│   ├── pandas&gt;=1.3.0\n│   ├── matplotlib&gt;=3.4.0\n│   └── seaborn&gt;=0.11.0\n│\n└── .git/                      # Version control"
  },
  {
    "objectID": "fiberL.html#use-cases",
    "href": "fiberL.html#use-cases",
    "title": "fiberL: Computer Vision for Fiber Analysis",
    "section": "Use Cases",
    "text": "Use Cases\n\nMaterials Science Research\n\nCharacterize electrospun nanofiber mats\nAnalyze polymer fiber networks\nStudy textile microstructure\nInvestigate biological fibrous materials (collagen, cellulose)\n\n\n\nQuality Control\n\nMonitor fiber production consistency\nDetect manufacturing defects\nQuantify batch-to-batch variation\nValidate material specifications\n\n\n\nAcademic Applications\n\nUndergraduate research projects\nGraduate thesis work\nPublication-quality quantitative analysis\nTeaching image processing concepts"
  },
  {
    "objectID": "fiberL.html#installation-usage",
    "href": "fiberL.html#installation-usage",
    "title": "fiberL: Computer Vision for Fiber Analysis",
    "section": "Installation & Usage",
    "text": "Installation & Usage\n\nInstallation\n# Clone repository\ngit clone https://github.com/lonespear/fiberL.git\ncd fiberL\n\n# Install dependencies\npip install -r requirements.txt\n\n\nRunning the Application\n# Launch Streamlit interface\nstreamlit run app.py\n\n\nProgrammatic Usage\nfrom fiberL import fiberL\nimport cv2\n\n# Load image\nimage = cv2.imread('fiber_image.tif', cv2.IMREAD_GRAYSCALE)\n\n# Initialize analyzer\nanalyzer = fiberL(\n    image=image,\n    scale_factor=0.5,  # microns per pixel\n    diffusion_iterations=10,\n    prune_length=5\n)\n\n# Process image\nanalyzer.process()\n\n# Get results\nstats = analyzer.get_statistics()\nprint(f\"Number of fibers: {stats['count']}\")\nprint(f\"Mean length: {stats['mean_length']:.2f} μm\")\nprint(f\"Mean persistence length: {stats['mean_persistence']:.2f} μm\")\n\n# Visualize\nanalyzer.plot_results()\n\n# Export data\nanalyzer.export_csv('fiber_data.csv')"
  },
  {
    "objectID": "fiberL.html#performance",
    "href": "fiberL.html#performance",
    "title": "fiberL: Computer Vision for Fiber Analysis",
    "section": "Performance",
    "text": "Performance\n\nComputational Complexity\n\n\n\nOperation\nTime Complexity\nSpace Complexity\n\n\n\n\nPreprocessing\nO(n²)\nO(n²)\n\n\nSkeletonization\nO(n² log n)\nO(n²)\n\n\nBranch Detection\nO(n²)\nO(n)\n\n\nNetwork Reconstruction\nO(m²)\nO(m)\n\n\nPersistence Length\nO(m³)\nO(m)\n\n\n\nWhere n = image dimensions, m = number of fiber points\n\n\nTypical Processing Times\n\n\n\nImage Size\nFiber Count\nProcessing Time\n\n\n\n\n512×512\n~50\n~2 seconds\n\n\n1024×1024\n~100\n~8 seconds\n\n\n2048×2048\n~200\n~30 seconds\n\n\n4096×4096\n~500\n~2 minutes\n\n\n\nTested on Intel i7-9700K, 32GB RAM"
  },
  {
    "objectID": "fiberL.html#limitations-future-work",
    "href": "fiberL.html#limitations-future-work",
    "title": "fiberL: Computer Vision for Fiber Analysis",
    "section": "Limitations & Future Work",
    "text": "Limitations & Future Work\n\nCurrent Limitations\n\nRequires high-contrast SEM images\nStruggles with highly overlapping fibers\nManual parameter tuning sometimes needed\n2D analysis only (no depth information)\n\n\n\nPlanned Enhancements\n\nDeep learning-based segmentation\n3D reconstruction from image stacks\nAutomated parameter optimization\nBatch processing improvements\nExport to standard formats (CSV, HDF5, JSON)\nIntegration with image databases"
  },
  {
    "objectID": "fiberL.html#research-applications",
    "href": "fiberL.html#research-applications",
    "title": "fiberL: Computer Vision for Fiber Analysis",
    "section": "Research Applications",
    "text": "Research Applications\nThis tool has been used in research investigating:\n\nPolymer Science: Electrospun nanofiber characterization\nTissue Engineering: Scaffold microstructure analysis\nMaterials Testing: Mechanical property prediction from structure\nBiomimetics: Natural fiber network quantification"
  },
  {
    "objectID": "fiberL.html#technical-documentation",
    "href": "fiberL.html#technical-documentation",
    "title": "fiberL: Computer Vision for Fiber Analysis",
    "section": "Technical Documentation",
    "text": "Technical Documentation\n\nKey Algorithms\n\nAnisotropic Diffusion (Perona-Malik)\n\\[\\frac{\\partial I}{\\partial t} = \\text{div}(c(|\\nabla I|) \\nabla I)\\]\nWhere diffusion coefficient: \\[c(|\\nabla I|) = \\exp\\left(-\\left(\\frac{|\\nabla I|}{K}\\right)^2\\right)\\]\n\n\nPersistence Length\n\\[\\langle \\cos\\theta(s) \\rangle = \\exp\\left(-\\frac{s}{L_p}\\right)\\]\nWhere: - \\(s\\) = arc length along fiber - \\(L_p\\) = persistence length - \\(\\theta(s)\\) = angle at position \\(s\\)\n\n\nCosine Similarity (Orientation Matching)\n\\[\\text{similarity} = \\frac{\\mathbf{v}_1 \\cdot \\mathbf{v}_2}{|\\mathbf{v}_1| |\\mathbf{v}_2|}\\]"
  },
  {
    "objectID": "fiberL.html#contributing",
    "href": "fiberL.html#contributing",
    "title": "fiberL: Computer Vision for Fiber Analysis",
    "section": "Contributing",
    "text": "Contributing\nContributions welcome! Areas for improvement: - Algorithm optimization - Additional quantitative metrics - Enhanced visualization options - Documentation improvements - Test coverage"
  },
  {
    "objectID": "fiberL.html#citation",
    "href": "fiberL.html#citation",
    "title": "fiberL: Computer Vision for Fiber Analysis",
    "section": "Citation",
    "text": "Citation\nIf you use fiberL in your research, please cite:\nDay, J. (2025). fiberL: Computer Vision for Fiber Network Analysis.\nGitHub repository: https://github.com/lonespear/fiberL"
  },
  {
    "objectID": "fiberL.html#contact",
    "href": "fiberL.html#contact",
    "title": "fiberL: Computer Vision for Fiber Analysis",
    "section": "Contact",
    "text": "Contact\nJonathan Day Department of Mathematical Sciences United States Military Academy West Point, NY\nEmail: jonathan.day@westpoint.edu GitHub: github.com/lonespear\n\n← Back to Projects View on GitHub"
  }
]